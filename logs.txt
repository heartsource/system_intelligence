* 
* ==> Audit <==
* |------------|----------------------------------|----------|-------------------------|---------|---------------------|---------------------|
|  Command   |               Args               | Profile  |          User           | Version |     Start Time      |      End Time       |
|------------|----------------------------------|----------|-------------------------|---------|---------------------|---------------------|
| start      |                                  | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 22:35 GMT | 06 Feb 24 22:38 GMT |
| addons     | enable ingress                   | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 22:39 GMT | 06 Feb 24 22:39 GMT |
| addons     | enable ingress                   | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 22:41 GMT | 06 Feb 24 22:41 GMT |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 22:46 GMT |                     |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 23:36 GMT |                     |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 23:45 GMT |                     |
| start      |                                  | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 23:46 GMT | 06 Feb 24 23:46 GMT |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 06 Feb 24 23:47 GMT |                     |
| start      |                                  | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 09:53 GMT |                     |
| docker-env |                                  | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 10:00 GMT |                     |
| start      |                                  | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 10:00 GMT | 07 Feb 24 10:01 GMT |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 10:36 GMT |                     |
| docker-env |                                  | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 10:37 GMT | 07 Feb 24 10:37 GMT |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 12:36 GMT | 07 Feb 24 12:36 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | myapp-service --url              | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 12:44 GMT |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 12:44 GMT | 07 Feb 24 12:56 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | list                             | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 12:56 GMT | 07 Feb 24 12:56 GMT |
| service    | status                           | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 12:56 GMT |                     |
| service    | list                             | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 12:57 GMT | 07 Feb 24 12:57 GMT |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 13:05 GMT | 07 Feb 24 13:09 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 22:42 GMT | 07 Feb 24 22:45 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 23:09 GMT | 07 Feb 24 23:23 GMT |
|            | --url                            |          |                         |         |                     |                     |
| start      |                                  | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 23:49 GMT | 07 Feb 24 23:51 GMT |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 07 Feb 24 23:59 GMT |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 00:03 GMT | 08 Feb 24 00:15 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 00:15 GMT | 08 Feb 24 00:18 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 00:52 GMT | 08 Feb 24 00:54 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 01:02 GMT | 08 Feb 24 01:56 GMT |
|            | --url                            |          |                         |         |                     |                     |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 01:57 GMT |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 02:06 GMT | 08 Feb 24 09:30 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 02:27 GMT | 08 Feb 24 08:38 GMT |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 08:39 GMT |                     |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 08:41 GMT |                     |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 08:58 GMT | 08 Feb 24 11:56 GMT |
|            | --url                            |          |                         |         |                     |                     |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 12:23 GMT |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 12:25 GMT |                     |
|            | --url                            |          |                         |         |                     |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 12:26 GMT | 08 Feb 24 12:33 GMT |
|            | --url                            |          |                         |         |                     |                     |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 12:28 GMT |                     |
| dashboard  |                                  | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 12:51 GMT |                     |
| service    | productsupportbackendapp-service | minikube | myilvahananduraipandian | v1.32.0 | 08 Feb 24 12:52 GMT |                     |
|            | --url                            |          |                         |         |                     |                     |
|------------|----------------------------------|----------|-------------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/02/07 23:49:53
Running on machine: Myilvahanans-MacBook-Pro
Binary: Built with gc go1.21.4 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0207 23:49:53.145620   43825 out.go:296] Setting OutFile to fd 1 ...
I0207 23:49:53.146473   43825 out.go:348] isatty.IsTerminal(1) = true
I0207 23:49:53.146476   43825 out.go:309] Setting ErrFile to fd 2...
I0207 23:49:53.146481   43825 out.go:348] isatty.IsTerminal(2) = true
I0207 23:49:53.146700   43825 root.go:338] Updating PATH: /Users/myilvahananduraipandian/.minikube/bin
W0207 23:49:53.146823   43825 root.go:314] Error reading config file at /Users/myilvahananduraipandian/.minikube/config/config.json: open /Users/myilvahananduraipandian/.minikube/config/config.json: no such file or directory
I0207 23:49:53.150831   43825 out.go:303] Setting JSON to false
I0207 23:49:53.196884   43825 start.go:128] hostinfo: {"hostname":"Myilvahanans-MacBook-Pro.local","uptime":1733108,"bootTime":1705616685,"procs":785,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.2.1","kernelVersion":"23.2.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"17a2cbb2-31d7-5df4-83af-93db72386780"}
W0207 23:49:53.196985   43825 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0207 23:49:53.224141   43825 out.go:177] üòÑ  minikube v1.32.0 on Darwin 14.2.1
I0207 23:49:53.267272   43825 notify.go:220] Checking for updates...
I0207 23:49:53.268331   43825 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0207 23:49:53.270641   43825 driver.go:378] Setting default libvirt URI to qemu:///system
I0207 23:49:53.604988   43825 docker.go:122] docker version: linux-20.10.23:Docker Desktop 4.17.0 (99724)
I0207 23:49:53.605546   43825 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0207 23:50:01.222887   43825 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (7.616624352s)
I0207 23:50:01.225124   43825 info.go:266] docker info: {ID:BNFB:MA5L:5D6V:BGXA:E222:PNPF:XLZX:7HKH:X6SU:GGOL:4VB4:HRZK Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:false NGoroutines:51 SystemTime:2024-02-07 23:49:53.752565552 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8346099712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.23 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2456e983eb9e37e47538f59ea18f2043c9a73640 Expected:2456e983eb9e37e47538f59ea18f2043c9a73640} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.15.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.18] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.25.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.6.0]] Warnings:<nil>}}
I0207 23:50:01.261103   43825 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0207 23:50:01.337018   43825 start.go:298] selected driver: docker
I0207 23:50:01.337386   43825 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0207 23:50:01.337542   43825 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0207 23:50:01.338087   43825 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0207 23:50:01.696060   43825 info.go:266] docker info: {ID:BNFB:MA5L:5D6V:BGXA:E222:PNPF:XLZX:7HKH:X6SU:GGOL:4VB4:HRZK Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:false NGoroutines:51 SystemTime:2024-02-07 23:50:01.482956829 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8346099712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.23 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2456e983eb9e37e47538f59ea18f2043c9a73640 Expected:2456e983eb9e37e47538f59ea18f2043c9a73640} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.3] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.15.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.18] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.25.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.6.0]] Warnings:<nil>}}
I0207 23:50:01.702455   43825 cni.go:84] Creating CNI manager for ""
I0207 23:50:01.702984   43825 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0207 23:50:01.703717   43825 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0207 23:50:01.728169   43825 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0207 23:50:01.771301   43825 cache.go:121] Beginning downloading kic base image for docker with docker
I0207 23:50:01.792193   43825 out.go:177] üöú  Pulling base image ...
I0207 23:50:01.833509   43825 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0207 23:50:01.833625   43825 preload.go:148] Found local preload: /Users/myilvahananduraipandian/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0207 23:50:01.833638   43825 cache.go:56] Caching tarball of preloaded images
I0207 23:50:01.833968   43825 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0207 23:50:01.836247   43825 preload.go:174] Found /Users/myilvahananduraipandian/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0207 23:50:01.836348   43825 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0207 23:50:01.838705   43825 profile.go:148] Saving config to /Users/myilvahananduraipandian/.minikube/profiles/minikube/config.json ...
I0207 23:50:01.942741   43825 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0207 23:50:01.942794   43825 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0207 23:50:01.943339   43825 cache.go:194] Successfully downloaded all kic artifacts
I0207 23:50:01.943429   43825 start.go:365] acquiring machines lock for minikube: {Name:mk1b0b273e775a36c47b9cf4f3e4e81f63f672cc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0207 23:50:01.944050   43825 start.go:369] acquired machines lock for "minikube" in 591.937¬µs
I0207 23:50:01.944078   43825 start.go:96] Skipping create...Using existing machine configuration
I0207 23:50:01.944593   43825 fix.go:54] fixHost starting: 
I0207 23:50:01.945530   43825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0207 23:50:02.037822   43825 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0207 23:50:02.037866   43825 fix.go:128] unexpected machine state, will restart: <nil>
I0207 23:50:02.062553   43825 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0207 23:50:02.104321   43825 cli_runner.go:164] Run: docker start minikube
I0207 23:50:03.202790   43825 cli_runner.go:217] Completed: docker start minikube: (1.098375934s)
I0207 23:50:03.202995   43825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0207 23:50:03.298091   43825 kic.go:430] container "minikube" state is running.
I0207 23:50:03.300407   43825 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0207 23:50:03.382565   43825 profile.go:148] Saving config to /Users/myilvahananduraipandian/.minikube/profiles/minikube/config.json ...
I0207 23:50:03.383046   43825 machine.go:88] provisioning docker machine ...
I0207 23:50:03.383991   43825 ubuntu.go:169] provisioning hostname "minikube"
I0207 23:50:03.384735   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:03.477380   43825 main.go:141] libmachine: Using SSH client type: native
I0207 23:50:03.478482   43825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 127.0.0.1 65201 <nil> <nil>}
I0207 23:50:03.478495   43825 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0207 23:50:03.496971   43825 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0207 23:50:07.049508   43825 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0207 23:50:07.050599   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:07.270276   43825 main.go:141] libmachine: Using SSH client type: native
I0207 23:50:07.271552   43825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 127.0.0.1 65201 <nil> <nil>}
I0207 23:50:07.271574   43825 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0207 23:50:07.844345   43825 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0207 23:50:07.844912   43825 ubuntu.go:175] set auth options {CertDir:/Users/myilvahananduraipandian/.minikube CaCertPath:/Users/myilvahananduraipandian/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/myilvahananduraipandian/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/myilvahananduraipandian/.minikube/machines/server.pem ServerKeyPath:/Users/myilvahananduraipandian/.minikube/machines/server-key.pem ClientKeyPath:/Users/myilvahananduraipandian/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/myilvahananduraipandian/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/myilvahananduraipandian/.minikube}
I0207 23:50:07.845035   43825 ubuntu.go:177] setting up certificates
I0207 23:50:07.845338   43825 provision.go:83] configureAuth start
I0207 23:50:07.845885   43825 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0207 23:50:08.031163   43825 provision.go:138] copyHostCerts
I0207 23:50:08.040892   43825 exec_runner.go:144] found /Users/myilvahananduraipandian/.minikube/ca.pem, removing ...
I0207 23:50:08.040911   43825 exec_runner.go:203] rm: /Users/myilvahananduraipandian/.minikube/ca.pem
I0207 23:50:08.042335   43825 exec_runner.go:151] cp: /Users/myilvahananduraipandian/.minikube/certs/ca.pem --> /Users/myilvahananduraipandian/.minikube/ca.pem (1123 bytes)
I0207 23:50:08.047495   43825 exec_runner.go:144] found /Users/myilvahananduraipandian/.minikube/cert.pem, removing ...
I0207 23:50:08.047506   43825 exec_runner.go:203] rm: /Users/myilvahananduraipandian/.minikube/cert.pem
I0207 23:50:08.047672   43825 exec_runner.go:151] cp: /Users/myilvahananduraipandian/.minikube/certs/cert.pem --> /Users/myilvahananduraipandian/.minikube/cert.pem (1168 bytes)
I0207 23:50:08.048548   43825 exec_runner.go:144] found /Users/myilvahananduraipandian/.minikube/key.pem, removing ...
I0207 23:50:08.048553   43825 exec_runner.go:203] rm: /Users/myilvahananduraipandian/.minikube/key.pem
I0207 23:50:08.050170   43825 exec_runner.go:151] cp: /Users/myilvahananduraipandian/.minikube/certs/key.pem --> /Users/myilvahananduraipandian/.minikube/key.pem (1675 bytes)
I0207 23:50:08.051739   43825 provision.go:112] generating server cert: /Users/myilvahananduraipandian/.minikube/machines/server.pem ca-key=/Users/myilvahananduraipandian/.minikube/certs/ca.pem private-key=/Users/myilvahananduraipandian/.minikube/certs/ca-key.pem org=myilvahananduraipandian.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0207 23:50:09.942328   43825 provision.go:172] copyRemoteCerts
I0207 23:50:09.946355   43825 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0207 23:50:09.947402   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:10.335347   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:50:10.490667   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1123 bytes)
I0207 23:50:10.568077   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/machines/server.pem --> /etc/docker/server.pem (1249 bytes)
I0207 23:50:10.635554   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0207 23:50:10.695269   43825 provision.go:86] duration metric: configureAuth took 2.849902521s
I0207 23:50:10.695306   43825 ubuntu.go:193] setting minikube options for container-runtime
I0207 23:50:10.695813   43825 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0207 23:50:10.695972   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:10.955222   43825 main.go:141] libmachine: Using SSH client type: native
I0207 23:50:10.955652   43825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 127.0.0.1 65201 <nil> <nil>}
I0207 23:50:10.955659   43825 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0207 23:50:11.183247   43825 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0207 23:50:11.183265   43825 ubuntu.go:71] root file system type: overlay
I0207 23:50:11.183865   43825 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0207 23:50:11.184163   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:11.408828   43825 main.go:141] libmachine: Using SSH client type: native
I0207 23:50:11.409465   43825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 127.0.0.1 65201 <nil> <nil>}
I0207 23:50:11.409563   43825 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0207 23:50:11.781750   43825 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0207 23:50:11.785184   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:12.112891   43825 main.go:141] libmachine: Using SSH client type: native
I0207 23:50:12.113980   43825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 127.0.0.1 65201 <nil> <nil>}
I0207 23:50:12.114007   43825 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0207 23:50:12.438248   43825 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0207 23:50:12.438287   43825 machine.go:91] provisioned docker machine in 9.055206231s
I0207 23:50:12.440529   43825 start.go:300] post-start starting for "minikube" (driver="docker")
I0207 23:50:12.440903   43825 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0207 23:50:12.441940   43825 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0207 23:50:12.442182   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:12.644335   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:50:12.799079   43825 ssh_runner.go:195] Run: cat /etc/os-release
I0207 23:50:12.814803   43825 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0207 23:50:12.815076   43825 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0207 23:50:12.815097   43825 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0207 23:50:12.815103   43825 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0207 23:50:12.815468   43825 filesync.go:126] Scanning /Users/myilvahananduraipandian/.minikube/addons for local assets ...
I0207 23:50:12.815745   43825 filesync.go:126] Scanning /Users/myilvahananduraipandian/.minikube/files for local assets ...
I0207 23:50:12.818012   43825 start.go:303] post-start completed in 377.412277ms
I0207 23:50:12.819166   43825 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0207 23:50:12.819477   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:13.020955   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:50:13.133480   43825 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0207 23:50:13.153576   43825 fix.go:56] fixHost completed within 11.209133127s
I0207 23:50:13.184668   43825 start.go:83] releasing machines lock for "minikube", held for 11.240565878s
I0207 23:50:13.185123   43825 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0207 23:50:13.312351   43825 ssh_runner.go:195] Run: cat /version.json
I0207 23:50:13.312508   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:13.313677   43825 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0207 23:50:13.314356   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:50:14.085802   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:50:14.137455   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:50:15.637957   43825 ssh_runner.go:235] Completed: cat /version.json: (2.325514104s)
I0207 23:50:15.638432   43825 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.324376548s)
I0207 23:50:15.643311   43825 ssh_runner.go:195] Run: systemctl --version
I0207 23:50:15.677397   43825 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0207 23:50:15.701663   43825 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0207 23:50:15.761689   43825 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0207 23:50:15.761908   43825 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0207 23:50:15.796980   43825 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0207 23:50:15.797011   43825 start.go:472] detecting cgroup driver to use...
I0207 23:50:15.797048   43825 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0207 23:50:15.800636   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0207 23:50:15.850874   43825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0207 23:50:15.912394   43825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0207 23:50:15.998802   43825 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0207 23:50:15.999178   43825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0207 23:50:16.072584   43825 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0207 23:50:16.102593   43825 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0207 23:50:16.125076   43825 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0207 23:50:16.161473   43825 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0207 23:50:16.190626   43825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0207 23:50:16.225280   43825 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0207 23:50:16.264142   43825 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0207 23:50:16.297791   43825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 23:50:16.579824   43825 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0207 23:50:17.056470   43825 start.go:472] detecting cgroup driver to use...
I0207 23:50:17.056499   43825 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0207 23:50:17.057376   43825 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0207 23:50:17.268770   43825 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0207 23:50:17.269055   43825 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0207 23:50:17.391307   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0207 23:50:17.441055   43825 ssh_runner.go:195] Run: which cri-dockerd
I0207 23:50:17.456773   43825 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0207 23:50:17.534695   43825 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0207 23:50:17.604679   43825 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0207 23:50:17.917938   43825 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0207 23:50:18.185195   43825 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0207 23:50:18.200632   43825 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0207 23:50:18.247422   43825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 23:50:18.489838   43825 ssh_runner.go:195] Run: sudo systemctl restart docker
I0207 23:50:20.909919   43825 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.420279065s)
I0207 23:50:20.911730   43825 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0207 23:50:21.206656   43825 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0207 23:50:21.422565   43825 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0207 23:50:21.661970   43825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 23:50:21.888786   43825 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0207 23:50:21.926474   43825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 23:50:22.151005   43825 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0207 23:50:23.323185   43825 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (1.172608449s)
I0207 23:50:23.323218   43825 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0207 23:50:23.327610   43825 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0207 23:50:23.391968   43825 start.go:540] Will wait 60s for crictl version
I0207 23:50:23.392225   43825 ssh_runner.go:195] Run: which crictl
I0207 23:50:23.435949   43825 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0207 23:50:23.866984   43825 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0207 23:50:23.867152   43825 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0207 23:50:24.069417   43825 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0207 23:50:24.369371   43825 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0207 23:50:24.371384   43825 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0207 23:50:26.428585   43825 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (2.057730234s)
I0207 23:50:26.429034   43825 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0207 23:50:26.434107   43825 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0207 23:50:26.447505   43825 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0207 23:50:26.574514   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0207 23:50:26.982602   43825 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0207 23:50:26.982758   43825 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0207 23:50:27.068801   43825 docker.go:671] Got preloaded images: -- stdout --
myil/productsupportbackendapi:1.1
nginx:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0207 23:50:27.069766   43825 docker.go:601] Images already preloaded, skipping extraction
I0207 23:50:27.070388   43825 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0207 23:50:27.226878   43825 docker.go:671] Got preloaded images: -- stdout --
myil/productsupportbackendapi:1.1
nginx:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --

I0207 23:50:27.229194   43825 cache_images.go:84] Images are preloaded, skipping loading
I0207 23:50:27.230445   43825 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0207 23:50:28.095236   43825 cni.go:84] Creating CNI manager for ""
I0207 23:50:28.095270   43825 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0207 23:50:28.096557   43825 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0207 23:50:28.097423   43825 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0207 23:50:28.109254   43825 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0207 23:50:28.130935   43825 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0207 23:50:28.143983   43825 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0207 23:50:28.235248   43825 binaries.go:44] Found k8s binaries, skipping transfer
I0207 23:50:28.235412   43825 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0207 23:50:28.276021   43825 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0207 23:50:28.373242   43825 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0207 23:50:28.446245   43825 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0207 23:50:28.516360   43825 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0207 23:50:28.533179   43825 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0207 23:50:28.590211   43825 certs.go:56] Setting up /Users/myilvahananduraipandian/.minikube/profiles/minikube for IP: 192.168.49.2
I0207 23:50:28.590367   43825 certs.go:190] acquiring lock for shared ca certs: {Name:mk812cbb6d750afd4b739b5ea7d8a09e1550e87e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0207 23:50:28.596249   43825 certs.go:199] skipping minikubeCA CA generation: /Users/myilvahananduraipandian/.minikube/ca.key
I0207 23:50:28.596923   43825 certs.go:199] skipping proxyClientCA CA generation: /Users/myilvahananduraipandian/.minikube/proxy-client-ca.key
I0207 23:50:28.597598   43825 certs.go:315] skipping minikube-user signed cert generation: /Users/myilvahananduraipandian/.minikube/profiles/minikube/client.key
I0207 23:50:28.601875   43825 certs.go:315] skipping minikube signed cert generation: /Users/myilvahananduraipandian/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0207 23:50:28.603059   43825 certs.go:315] skipping aggregator signed cert generation: /Users/myilvahananduraipandian/.minikube/profiles/minikube/proxy-client.key
I0207 23:50:28.609775   43825 certs.go:437] found cert: /Users/myilvahananduraipandian/.minikube/certs/Users/myilvahananduraipandian/.minikube/certs/ca-key.pem (1679 bytes)
I0207 23:50:28.612191   43825 certs.go:437] found cert: /Users/myilvahananduraipandian/.minikube/certs/Users/myilvahananduraipandian/.minikube/certs/ca.pem (1123 bytes)
I0207 23:50:28.612934   43825 certs.go:437] found cert: /Users/myilvahananduraipandian/.minikube/certs/Users/myilvahananduraipandian/.minikube/certs/cert.pem (1168 bytes)
I0207 23:50:28.613822   43825 certs.go:437] found cert: /Users/myilvahananduraipandian/.minikube/certs/Users/myilvahananduraipandian/.minikube/certs/key.pem (1675 bytes)
I0207 23:50:28.632212   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0207 23:50:28.741490   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0207 23:50:28.835883   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0207 23:50:28.931936   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0207 23:50:29.024787   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0207 23:50:29.108053   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0207 23:50:29.183869   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0207 23:50:29.254989   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0207 23:50:29.381091   43825 ssh_runner.go:362] scp /Users/myilvahananduraipandian/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0207 23:50:29.456070   43825 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0207 23:50:29.547057   43825 ssh_runner.go:195] Run: openssl version
I0207 23:50:29.591346   43825 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0207 23:50:29.619595   43825 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0207 23:50:29.630678   43825 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb  6 22:38 /usr/share/ca-certificates/minikubeCA.pem
I0207 23:50:29.630777   43825 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0207 23:50:29.650455   43825 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0207 23:50:29.686508   43825 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0207 23:50:29.706268   43825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0207 23:50:29.742561   43825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0207 23:50:29.761006   43825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0207 23:50:29.813684   43825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0207 23:50:29.868589   43825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0207 23:50:29.894280   43825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0207 23:50:29.919514   43825 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0207 23:50:29.920047   43825 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0207 23:50:30.035463   43825 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0207 23:50:30.075563   43825 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0207 23:50:30.076173   43825 kubeadm.go:636] restartCluster start
I0207 23:50:30.076409   43825 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0207 23:50:30.101642   43825 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0207 23:50:30.101868   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0207 23:50:30.507607   43825 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:50200"
I0207 23:50:30.507804   43825 kubeconfig.go:135] verify returned: got: 127.0.0.1:50200, want: 127.0.0.1:65200
I0207 23:50:30.508816   43825 lock.go:35] WriteFile acquiring /Users/myilvahananduraipandian/.kube/config: {Name:mk0d852b076581201d96f31c88a78783b9975ebb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0207 23:50:30.515774   43825 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0207 23:50:30.548016   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:30.548235   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:30.600906   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:30.600919   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:30.601206   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:30.652304   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:31.153055   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:31.153314   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:31.189659   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:31.653222   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:31.654679   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:31.714491   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:32.152306   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:32.152593   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:32.190305   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:32.653517   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:32.655469   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:32.732011   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:33.152360   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:33.191277   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:33.233263   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:33.652027   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:33.652324   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:33.684278   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:34.152025   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:34.153229   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:34.199669   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:34.651637   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:34.651819   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:34.686297   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:35.152578   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:35.152921   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:35.200638   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:35.651529   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:35.652605   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:35.714522   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:36.151487   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:36.151754   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:36.254349   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:36.651342   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:36.651624   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:36.703575   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:37.151884   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:37.153164   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:37.213882   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:37.651181   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:37.651421   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:37.691625   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:38.151066   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:38.161593   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:38.210683   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:38.651079   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:38.651588   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:38.678711   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:39.151520   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:39.151676   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:39.181081   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:39.651123   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:39.651296   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:39.667022   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:40.150814   43825 api_server.go:166] Checking apiserver status ...
I0207 23:50:40.151083   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 23:50:40.175218   43825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 23:50:40.546665   43825 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0207 23:50:40.547016   43825 kubeadm.go:1128] stopping kube-system containers ...
I0207 23:50:40.547170   43825 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0207 23:50:40.603425   43825 docker.go:469] Stopping containers: [142bcd1e1685 0783ada1eed1 6dcafa9f13fc 746228c9c10e 2a50946d338c ea97b06cc5da 533083f34ba1 0b08d1069232 37744438c907 98517ad6c199 28547570b9d4 ecb68b099a5b 4aac1280ed2d 3725212f51ba 4191535c336f 8b21e5f34b6b e795957978e1 e668a2f0c063 021a133bcf36 62983d6bcdea 0cfffa05b08e 035b25f99ea9 c790004d0d39 cf5c5dc5436c dac64d9e8242 1f5f169a0f33 2ccdd662f5f7]
I0207 23:50:40.603654   43825 ssh_runner.go:195] Run: docker stop 142bcd1e1685 0783ada1eed1 6dcafa9f13fc 746228c9c10e 2a50946d338c ea97b06cc5da 533083f34ba1 0b08d1069232 37744438c907 98517ad6c199 28547570b9d4 ecb68b099a5b 4aac1280ed2d 3725212f51ba 4191535c336f 8b21e5f34b6b e795957978e1 e668a2f0c063 021a133bcf36 62983d6bcdea 0cfffa05b08e 035b25f99ea9 c790004d0d39 cf5c5dc5436c dac64d9e8242 1f5f169a0f33 2ccdd662f5f7
I0207 23:50:40.652639   43825 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0207 23:50:40.681314   43825 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0207 23:50:40.698043   43825 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Feb  6 22:38 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Feb  7 10:00 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Feb  6 22:38 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Feb  7 10:00 /etc/kubernetes/scheduler.conf

I0207 23:50:40.698608   43825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0207 23:50:40.718927   43825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0207 23:50:40.734820   43825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0207 23:50:40.749685   43825 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0207 23:50:40.749811   43825 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0207 23:50:40.772225   43825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0207 23:50:40.791141   43825 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0207 23:50:40.791267   43825 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0207 23:50:40.809760   43825 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0207 23:50:40.833801   43825 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0207 23:50:40.834079   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0207 23:50:41.196364   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0207 23:50:43.055263   43825 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.8590812s)
I0207 23:50:43.055287   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0207 23:50:43.292774   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0207 23:50:43.400709   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0207 23:50:43.482428   43825 api_server.go:52] waiting for apiserver process to appear ...
I0207 23:50:43.483731   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:50:43.502642   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:50:44.022913   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:50:44.524232   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:50:45.022831   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:50:45.522781   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:50:46.022682   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:50:46.042088   43825 api_server.go:72] duration metric: took 2.559898324s to wait for apiserver process to appear ...
I0207 23:50:46.042100   43825 api_server.go:88] waiting for apiserver healthz status ...
I0207 23:50:46.042156   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:46.047411   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:46.047449   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:46.051982   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:46.552853   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:46.559862   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:47.052734   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:47.060571   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:47.552649   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:47.589175   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:48.052596   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:48.114680   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:48.555620   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:48.568507   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:49.052524   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:49.057613   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": EOF
I0207 23:50:49.552462   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:54.552606   43825 api_server.go:269] stopped: https://127.0.0.1:65200/healthz: Get "https://127.0.0.1:65200/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0207 23:50:54.552662   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:55.647764   43825 api_server.go:279] https://127.0.0.1:65200/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0207 23:50:55.648424   43825 api_server.go:103] status: https://127.0.0.1:65200/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0207 23:50:55.648478   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:55.741719   43825 api_server.go:279] https://127.0.0.1:65200/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0207 23:50:55.741765   43825 api_server.go:103] status: https://127.0.0.1:65200/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0207 23:50:56.052116   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:56.066221   43825 api_server.go:279] https://127.0.0.1:65200/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0207 23:50:56.066245   43825 api_server.go:103] status: https://127.0.0.1:65200/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0207 23:50:56.552186   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:56.565131   43825 api_server.go:279] https://127.0.0.1:65200/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0207 23:50:56.565154   43825 api_server.go:103] status: https://127.0.0.1:65200/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0207 23:50:57.051526   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:57.078836   43825 api_server.go:279] https://127.0.0.1:65200/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0207 23:50:57.078867   43825 api_server.go:103] status: https://127.0.0.1:65200/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0207 23:50:57.551969   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:50:57.564199   43825 api_server.go:279] https://127.0.0.1:65200/healthz returned 200:
ok
I0207 23:50:57.585012   43825 api_server.go:141] control plane version: v1.28.3
I0207 23:50:57.585032   43825 api_server.go:131] duration metric: took 11.543601491s to wait for apiserver health ...
I0207 23:50:57.585062   43825 cni.go:84] Creating CNI manager for ""
I0207 23:50:57.585698   43825 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0207 23:50:57.627822   43825 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0207 23:50:57.648749   43825 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0207 23:50:57.738055   43825 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0207 23:50:57.785505   43825 system_pods.go:43] waiting for kube-system pods to appear ...
I0207 23:50:57.850968   43825 system_pods.go:59] 7 kube-system pods found
I0207 23:50:57.850991   43825 system_pods.go:61] "coredns-5dd5756b68-tzbkr" [fab28c9e-dedb-4627-b3a8-9232a65a1729] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0207 23:50:57.851002   43825 system_pods.go:61] "etcd-minikube" [ce016f34-b796-4a97-bc6a-9aebe0f7f5b6] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0207 23:50:57.851020   43825 system_pods.go:61] "kube-apiserver-minikube" [52923799-0e83-4654-9f37-1ae65ca25e11] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0207 23:50:57.851026   43825 system_pods.go:61] "kube-controller-manager-minikube" [fbc960dd-c6bc-4edb-814b-f8d7e4b404af] Running
I0207 23:50:57.851040   43825 system_pods.go:61] "kube-proxy-qt5dx" [0229ccaa-b4d3-49ca-bd18-e8d5114014bd] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0207 23:50:57.851047   43825 system_pods.go:61] "kube-scheduler-minikube" [93b8b0c3-bfc7-4bc0-98ec-90e85a5118ef] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0207 23:50:57.851052   43825 system_pods.go:61] "storage-provisioner" [edbf28e9-077f-491d-81ec-f55995c2ff7f] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0207 23:50:57.851057   43825 system_pods.go:74] duration metric: took 65.544889ms to wait for pod list to return data ...
I0207 23:50:57.851429   43825 node_conditions.go:102] verifying NodePressure condition ...
I0207 23:50:57.860917   43825 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0207 23:50:57.860938   43825 node_conditions.go:123] node cpu capacity is 4
I0207 23:50:57.861293   43825 node_conditions.go:105] duration metric: took 9.855641ms to run NodePressure ...
I0207 23:50:57.861311   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0207 23:50:58.745026   43825 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0207 23:50:58.784190   43825 ops.go:34] apiserver oom_adj: -16
I0207 23:50:58.785121   43825 kubeadm.go:640] restartCluster took 28.71112993s
I0207 23:50:58.785343   43825 kubeadm.go:406] StartCluster complete in 28.868841273s
I0207 23:50:58.786180   43825 settings.go:142] acquiring lock: {Name:mk6838d09a0378d425618333a3a68d9733c5c122 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0207 23:50:58.788274   43825 settings.go:150] Updating kubeconfig:  /Users/myilvahananduraipandian/.kube/config
I0207 23:50:58.791979   43825 lock.go:35] WriteFile acquiring /Users/myilvahananduraipandian/.kube/config: {Name:mk0d852b076581201d96f31c88a78783b9975ebb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0207 23:50:58.794954   43825 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0207 23:50:58.795471   43825 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0207 23:50:58.796105   43825 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0207 23:50:58.796147   43825 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0207 23:50:58.796168   43825 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0207 23:50:58.796178   43825 addons.go:69] Setting dashboard=true in profile "minikube"
I0207 23:50:58.796180   43825 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0207 23:50:58.796184   43825 addons.go:240] addon storage-provisioner should already be in state true
I0207 23:50:58.796189   43825 addons.go:231] Setting addon dashboard=true in "minikube"
W0207 23:50:58.796194   43825 addons.go:240] addon dashboard should already be in state true
I0207 23:50:58.796200   43825 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0207 23:50:58.796251   43825 addons.go:69] Setting ingress=true in profile "minikube"
I0207 23:50:58.796283   43825 addons.go:231] Setting addon ingress=true in "minikube"
W0207 23:50:58.796293   43825 addons.go:240] addon ingress should already be in state true
I0207 23:50:58.797248   43825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0207 23:50:58.797479   43825 host.go:66] Checking if "minikube" exists ...
I0207 23:50:58.797484   43825 host.go:66] Checking if "minikube" exists ...
I0207 23:50:58.797488   43825 host.go:66] Checking if "minikube" exists ...
I0207 23:50:58.798027   43825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0207 23:50:58.798070   43825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0207 23:50:58.798108   43825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0207 23:50:58.845157   43825 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0207 23:50:58.845233   43825 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0207 23:50:58.865567   43825 out.go:177] üîé  Verifying Kubernetes components...
I0207 23:50:58.903577   43825 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0207 23:51:00.589588   43825 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.791520171s)
I0207 23:51:00.615853   43825 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0207 23:51:00.643520   43825 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.84541903s)
I0207 23:51:00.643593   43825 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.845467432s)
I0207 23:51:00.643640   43825 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.846389684s)
I0207 23:51:00.648682   43825 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0207 23:51:00.694333   43825 out.go:177] üí°  After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0207 23:51:00.696500   43825 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0207 23:51:00.721236   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0207 23:51:00.721307   43825 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
W0207 23:51:00.721282   43825 addons.go:240] addon default-storageclass should already be in state true
I0207 23:51:00.721571   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:51:00.870455   43825 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/controller:v1.9.4
I0207 23:51:00.808335   43825 host.go:66] Checking if "minikube" exists ...
I0207 23:51:00.936268   43825 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0207 23:51:00.910273   43825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0207 23:51:00.967548   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0207 23:51:01.010551   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0207 23:51:01.010812   43825 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0207 23:51:01.010931   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:51:01.101977   43825 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0207 23:51:01.135522   43825 addons.go:423] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0207 23:51:01.135537   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16103 bytes)
I0207 23:51:01.135689   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:51:01.184159   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:51:01.230480   43825 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0207 23:51:01.230494   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0207 23:51:01.230712   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0207 23:51:01.388526   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:51:01.416867   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:51:01.417133   43825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:65201 SSHKeyPath:/Users/myilvahananduraipandian/.minikube/machines/minikube/id_rsa Username:docker}
I0207 23:51:01.860578   43825 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0207 23:51:02.032356   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0207 23:51:02.032379   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0207 23:51:02.034448   43825 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0207 23:51:02.034448   43825 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0207 23:51:02.479776   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0207 23:51:02.479791   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0207 23:51:03.347072   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0207 23:51:03.347090   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0207 23:51:04.862133   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0207 23:51:04.862145   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0207 23:51:05.372546   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0207 23:51:05.372625   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0207 23:51:06.390745   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0207 23:51:06.390797   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0207 23:51:06.939635   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0207 23:51:06.939651   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0207 23:51:07.064536   43825 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (8.269606352s)
I0207 23:51:07.064692   43825 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (8.161281858s)
I0207 23:51:07.065190   43825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0207 23:51:07.065278   43825 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0207 23:51:07.368390   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0207 23:51:07.368425   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0207 23:51:07.516674   43825 api_server.go:52] waiting for apiserver process to appear ...
I0207 23:51:07.516975   43825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 23:51:07.656787   43825 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0207 23:51:07.656812   43825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0207 23:51:07.906885   43825 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0207 23:51:13.765716   43825 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (11.905029688s)
I0207 23:51:14.784086   43825 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (12.749778227s)
I0207 23:51:14.784155   43825 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (12.74987818s)
I0207 23:51:14.784316   43825 addons.go:467] Verifying addon ingress=true in "minikube"
I0207 23:51:14.784411   43825 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (7.267487147s)
I0207 23:51:14.784463   43825 api_server.go:72] duration metric: took 15.939536998s to wait for apiserver process to appear ...
I0207 23:51:14.784490   43825 api_server.go:88] waiting for apiserver healthz status ...
I0207 23:51:14.784526   43825 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:65200/healthz ...
I0207 23:51:14.842769   43825 out.go:177] üîé  Verifying ingress addon...
I0207 23:51:14.785681   43825 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (6.877812306s)
I0207 23:51:14.801078   43825 api_server.go:279] https://127.0.0.1:65200/healthz returned 200:
ok
I0207 23:51:14.924525   43825 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0207 23:51:14.952789   43825 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0207 23:51:14.926400   43825 api_server.go:141] control plane version: v1.28.3
I0207 23:51:14.952873   43825 api_server.go:131] duration metric: took 168.372463ms to wait for apiserver health ...
I0207 23:51:14.964508   43825 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0207 23:51:15.047665   43825 system_pods.go:43] waiting for kube-system pods to appear ...
I0207 23:51:15.047668   43825 kapi.go:107] duration metric: took 123.205768ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0207 23:51:15.077949   43825 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, dashboard, ingress
I0207 23:51:15.061142   43825 system_pods.go:59] 7 kube-system pods found
I0207 23:51:15.157791   43825 addons.go:502] enable addons completed in 16.362542597s: enabled=[storage-provisioner default-storageclass dashboard ingress]
I0207 23:51:15.078013   43825 system_pods.go:61] "coredns-5dd5756b68-tzbkr" [fab28c9e-dedb-4627-b3a8-9232a65a1729] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0207 23:51:15.157828   43825 system_pods.go:61] "etcd-minikube" [ce016f34-b796-4a97-bc6a-9aebe0f7f5b6] Running
I0207 23:51:15.157834   43825 system_pods.go:61] "kube-apiserver-minikube" [52923799-0e83-4654-9f37-1ae65ca25e11] Running
I0207 23:51:15.157846   43825 system_pods.go:61] "kube-controller-manager-minikube" [fbc960dd-c6bc-4edb-814b-f8d7e4b404af] Running
I0207 23:51:15.157878   43825 system_pods.go:61] "kube-proxy-qt5dx" [0229ccaa-b4d3-49ca-bd18-e8d5114014bd] Running
I0207 23:51:15.157882   43825 system_pods.go:61] "kube-scheduler-minikube" [93b8b0c3-bfc7-4bc0-98ec-90e85a5118ef] Running
I0207 23:51:15.157886   43825 system_pods.go:61] "storage-provisioner" [edbf28e9-077f-491d-81ec-f55995c2ff7f] Running
I0207 23:51:15.157890   43825 system_pods.go:74] duration metric: took 110.217734ms to wait for pod list to return data ...
I0207 23:51:15.157907   43825 kubeadm.go:581] duration metric: took 16.312974945s to wait for : map[apiserver:true system_pods:true] ...
I0207 23:51:15.157919   43825 node_conditions.go:102] verifying NodePressure condition ...
I0207 23:51:15.163288   43825 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0207 23:51:15.163300   43825 node_conditions.go:123] node cpu capacity is 4
I0207 23:51:15.163307   43825 node_conditions.go:105] duration metric: took 5.384924ms to run NodePressure ...
I0207 23:51:15.163315   43825 start.go:228] waiting for startup goroutines ...
I0207 23:51:15.163320   43825 start.go:233] waiting for cluster config update ...
I0207 23:51:15.163342   43825 start.go:242] writing updated cluster config ...
I0207 23:51:15.168123   43825 ssh_runner.go:195] Run: rm -f paused
I0207 23:51:15.264672   43825 start.go:600] kubectl: 1.25.4, cluster: 1.28.3 (minor skew: 3)
I0207 23:51:15.294650   43825 out.go:177] 
W0207 23:51:15.325961   43825 out.go:239] ‚ùó  /usr/local/bin/kubectl is version 1.25.4, which may have incompatibilities with Kubernetes 1.28.3.
I0207 23:51:15.355593   43825 out.go:177]     ‚ñ™ Want kubectl v1.28.3? Try 'minikube kubectl -- get pods -A'
I0207 23:51:15.416806   43825 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Feb 07 23:50:45 minikube cri-dockerd[1074]: time="2024-02-07T23:50:45Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-ckjl7_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"517122942ee86524577d538781f5d0f9b93dd85e68b3f74b2594a9a81e5feba4\""
Feb 07 23:50:45 minikube cri-dockerd[1074]: time="2024-02-07T23:50:45Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ingress-nginx-controller-7c6974c4d8-9wzk7_ingress-nginx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cd06d0a3ea33d0d6b2a0154cd3cba27590654a8fe39f0da03dda22cbd8263ad2\""
Feb 07 23:50:45 minikube cri-dockerd[1074]: time="2024-02-07T23:50:45Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-tzbkr_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2a50946d338c8d364feef8cae58e3ee34e264969fc707510f012f52d6219347e\""
Feb 07 23:50:45 minikube cri-dockerd[1074]: time="2024-02-07T23:50:45Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"my-nginx-7fbf685c4d-tsq5b_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"dd1eaebddc8bee003e70c7979a1fa4be4a2f59fb9e3d3b4f1085c43cfb46b576\""
Feb 07 23:50:55 minikube cri-dockerd[1074]: time="2024-02-07T23:50:55Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 07 23:50:56 minikube cri-dockerd[1074]: time="2024-02-07T23:50:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/720c407a70463f1f3c681449577326231d9f689889d557366b708dc358957d55/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Feb 07 23:50:57 minikube cri-dockerd[1074]: time="2024-02-07T23:50:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/617b37a7b86c1fe0bd3c0880556667ba8b074a62881fc532376fd0041297ab78/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Feb 07 23:50:57 minikube cri-dockerd[1074]: time="2024-02-07T23:50:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a7fb9ad61af2783a9836a192aad9d60d0c15bb32c13c0927405aecdc431e8a0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 07 23:50:58 minikube cri-dockerd[1074]: time="2024-02-07T23:50:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/84fa73ebbe519e72ec5f5d55380936d6e7c67ff4b651e39f2b46db384e2c7ba2/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 07 23:50:58 minikube cri-dockerd[1074]: time="2024-02-07T23:50:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/429f591fb7aa4ccab346e2eba8f83b7ec58aa7e9ef27333179d6746e3e0db61d/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 07 23:50:58 minikube cri-dockerd[1074]: time="2024-02-07T23:50:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ab9076a6b93fe6a0326290085f8a037623b4da6c5def9354816961898c2d1d12/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 07 23:50:58 minikube cri-dockerd[1074]: time="2024-02-07T23:50:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb8fd63d12966f5fcb814c43d2ba89787fdfec25a38a943b35774f6ec0c9dc44/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Feb 07 23:51:02 minikube cri-dockerd[1074]: time="2024-02-07T23:51:02Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 07 23:51:28 minikube dockerd[810]: time="2024-02-07T23:51:28.175950218Z" level=info msg="ignoring event" container=9cf92bbaac020f4798a744612a9b25744ab21726f78b6cefb376ca7edbb58942 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 07 23:51:33 minikube dockerd[810]: time="2024-02-07T23:51:33.544915889Z" level=info msg="ignoring event" container=7f67e3f5cccd73f7c271db6f81c068377e85fca18919cbcfad1d5f3477ad4f15 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 07 23:59:05 minikube cri-dockerd[1074]: time="2024-02-07T23:59:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/71f42fb2213a89ef63153ed38b44ae6c06be00d6fe9c5611ff2445489a82c5a8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 00:00:51 minikube dockerd[810]: time="2024-02-08T00:00:51.422157774Z" level=info msg="ignoring event" container=a0db26a41f4a7aa6ab62d0abc61f53731ac43b0bbe3e8621466ed1b2e8b52d69 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:00:51 minikube dockerd[810]: time="2024-02-08T00:00:51.555121722Z" level=info msg="ignoring event" container=71f42fb2213a89ef63153ed38b44ae6c06be00d6fe9c5611ff2445489a82c5a8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:00:52 minikube cri-dockerd[1074]: time="2024-02-08T00:00:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/03be9be23417708ad907946bc84dfd03241f89f759b028792377ccbdb90daa06/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 00:01:08 minikube dockerd[810]: time="2024-02-08T00:01:08.404928237Z" level=info msg="ignoring event" container=fb59f28eaef3a1b9ac00dcbea50809466b6fbe9e5f5913d9d7f6e56c8094a43b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:01:08 minikube dockerd[810]: time="2024-02-08T00:01:08.501786116Z" level=info msg="ignoring event" container=03be9be23417708ad907946bc84dfd03241f89f759b028792377ccbdb90daa06 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:01:09 minikube cri-dockerd[1074]: time="2024-02-08T00:01:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6ac2feeb7b278d93c4d359f16fc1fbeeabe56ee6b5d8ee8e66b93563e679a7d1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 00:01:46 minikube dockerd[810]: time="2024-02-08T00:01:46.941490339Z" level=info msg="ignoring event" container=2d7f1b5313366f0935e3e2b64ff46cfeaf80b3043de961699f0675252aac55e0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:01:47 minikube dockerd[810]: time="2024-02-08T00:01:47.023571468Z" level=info msg="ignoring event" container=6ac2feeb7b278d93c4d359f16fc1fbeeabe56ee6b5d8ee8e66b93563e679a7d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:01:47 minikube cri-dockerd[1074]: time="2024-02-08T00:01:47Z" level=error msg="Error response from daemon: No such container: 2d7f1b5313366f0935e3e2b64ff46cfeaf80b3043de961699f0675252aac55e0 Failed to get stats from container 2d7f1b5313366f0935e3e2b64ff46cfeaf80b3043de961699f0675252aac55e0"
Feb 08 00:01:48 minikube cri-dockerd[1074]: time="2024-02-08T00:01:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb60b45b8fd73ce777beee2c227481b348e65839305ca09cb84973d1ad845e5b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 00:02:10 minikube dockerd[810]: time="2024-02-08T00:02:10.127035012Z" level=info msg="ignoring event" container=a166d04ae9721bba6b747ccd4bc683d3f23e26717463a281f1b33f02b5c410b7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:02:10 minikube cri-dockerd[1074]: time="2024-02-08T00:02:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/056c6f375cc6c98722fba0474d8a9c390eee28dd6e7a2a217ab0e39887f61cb5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 00:02:10 minikube dockerd[810]: time="2024-02-08T00:02:10.234882413Z" level=info msg="ignoring event" container=eb60b45b8fd73ce777beee2c227481b348e65839305ca09cb84973d1ad845e5b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:02:35 minikube dockerd[810]: time="2024-02-08T00:02:35.685068872Z" level=info msg="ignoring event" container=47a7fe1f381d714f53de577b6ccf0d3309b642b4be4ccefae8a0973ed5aa6c0c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:02:35 minikube dockerd[810]: time="2024-02-08T00:02:35.836630065Z" level=info msg="ignoring event" container=056c6f375cc6c98722fba0474d8a9c390eee28dd6e7a2a217ab0e39887f61cb5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:02:35 minikube cri-dockerd[1074]: time="2024-02-08T00:02:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dbf84dd1f10d2472c09ffe493e5fc28be6e030e524ee256bcb39161b7cd8d0ea/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 00:51:23 minikube dockerd[810]: time="2024-02-08T00:51:23.884324975Z" level=info msg="ignoring event" container=648a4862d223b9c8c09da40feccb59b01eccb4e3c115b5ca085967a110741fdf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:51:24 minikube dockerd[810]: time="2024-02-08T00:51:24.088728618Z" level=info msg="ignoring event" container=dbf84dd1f10d2472c09ffe493e5fc28be6e030e524ee256bcb39161b7cd8d0ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 00:51:39 minikube cri-dockerd[1074]: time="2024-02-08T00:51:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ffb3e40e76c1bb09000f8e51cdc2ade9ceced6d74b2789a1e48a0f3422586596/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 01:00:53 minikube dockerd[810]: time="2024-02-08T01:00:53.966800765Z" level=info msg="ignoring event" container=38047d59d690d0be24f5e2d3d94dbf01cbf4f06d13a2c1925ebc27bb8c4289ac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 01:00:54 minikube dockerd[810]: time="2024-02-08T01:00:54.118398837Z" level=info msg="ignoring event" container=ffb3e40e76c1bb09000f8e51cdc2ade9ceced6d74b2789a1e48a0f3422586596 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 01:01:09 minikube cri-dockerd[1074]: time="2024-02-08T01:01:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c8bf567d70093103c8b6a64ba930310531e83096d67a8a8a5207a3cc3a9f04cd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 01:58:13 minikube dockerd[810]: time="2024-02-08T01:58:13.023149209Z" level=info msg="ignoring event" container=68ff166e0b4a99c9facb79644449f8c8295f466b15411c4e82d55d84ee92885e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 01:58:13 minikube dockerd[810]: time="2024-02-08T01:58:13.290558667Z" level=info msg="ignoring event" container=c8bf567d70093103c8b6a64ba930310531e83096d67a8a8a5207a3cc3a9f04cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 02:03:40 minikube cri-dockerd[1074]: time="2024-02-08T02:03:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/feea62c294875b86297fc764335bd5ebf34afc977225dd431899d4e6397c10e2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 02:04:20 minikube dockerd[810]: time="2024-02-08T02:04:20.815221044Z" level=info msg="ignoring event" container=be69e19128929c09399b82f0026c593920e473213bdf4761a9c777309526d45e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 02:04:20 minikube dockerd[810]: time="2024-02-08T02:04:20.962565710Z" level=info msg="ignoring event" container=feea62c294875b86297fc764335bd5ebf34afc977225dd431899d4e6397c10e2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 02:04:37 minikube cri-dockerd[1074]: time="2024-02-08T02:04:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5528eae5f74c00b22919b57fada59e58670fea2efe6e6b84d07dbdf8a622fe4d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 02:21:42 minikube dockerd[810]: time="2024-02-08T02:21:42.632369372Z" level=info msg="ignoring event" container=ebe3c3f8877cccc8b228a6ad5f708bbf5950a209413093c04db6fed2988a8deb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 02:21:42 minikube dockerd[810]: time="2024-02-08T02:21:42.845744914Z" level=info msg="ignoring event" container=5528eae5f74c00b22919b57fada59e58670fea2efe6e6b84d07dbdf8a622fe4d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 02:25:32 minikube cri-dockerd[1074]: time="2024-02-08T02:25:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f93ac071b38f307e6e6cfae4b298ec1c9bde13df94ae3f8622956ac2d8608742/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 06:36:55 minikube dockerd[810]: time="2024-02-08T06:36:55.949717985Z" level=info msg="ignoring event" container=affbabedbdac1eef50a7d5742b7af9ec658cd2f63eb720f989dcebc1027f93ac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 06:37:16 minikube dockerd[810]: time="2024-02-08T06:37:16.843959512Z" level=info msg="ignoring event" container=fce03fb13ee86e9cf422f3a36c87240823521f268749d9f881052d9920654d2e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 08:47:24 minikube cri-dockerd[1074]: time="2024-02-08T08:47:24Z" level=error msg="Error listing containers with filter: &ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
Feb 08 08:47:24 minikube cri-dockerd[1074]: time="2024-02-08T08:47:24Z" level=error msg="Error listing containers error: operation timeout: context deadline exceeded"
Feb 08 09:00:28 minikube dockerd[810]: time="2024-02-08T09:00:28.849380492Z" level=info msg="ignoring event" container=0cb34699169e3c8baeb8afdfc4e85d61b7408df4743da776e34b12df271b5e7a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 09:00:29 minikube cri-dockerd[1074]: time="2024-02-08T09:00:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dd4494973a502e7b12b47534c6983bf479466825fb8b4f62785c4be19b32e411/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 09:00:29 minikube dockerd[810]: time="2024-02-08T09:00:29.184681849Z" level=info msg="ignoring event" container=f93ac071b38f307e6e6cfae4b298ec1c9bde13df94ae3f8622956ac2d8608742 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 12:23:49 minikube dockerd[810]: time="2024-02-08T12:23:49.090170034Z" level=info msg="ignoring event" container=0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 12:23:49 minikube dockerd[810]: time="2024-02-08T12:23:49.396678345Z" level=info msg="ignoring event" container=dd4494973a502e7b12b47534c6983bf479466825fb8b4f62785c4be19b32e411 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 12:25:35 minikube cri-dockerd[1074]: time="2024-02-08T12:25:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8afe70952b784e203dc95dfc48e3905f26ed632bc3c21128a07f26e8c7fcae53/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 12:51:29 minikube dockerd[810]: time="2024-02-08T12:51:29.500603127Z" level=info msg="ignoring event" container=4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 12:51:29 minikube dockerd[810]: time="2024-02-08T12:51:29.634727926Z" level=info msg="ignoring event" container=8afe70952b784e203dc95dfc48e3905f26ed632bc3c21128a07f26e8c7fcae53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 12:52:13 minikube cri-dockerd[1074]: time="2024-02-08T12:52:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f38948a02ef8f894dc96e455eff330c5b435d0f7ef807c07bbe8661eb81e4c0d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
4c0a7fa82cc3d       db2a94d3dd8be                                                                                                                24 seconds ago      Running             productsupportbackendapp    0                   f38948a02ef8f       productsupportbackendapp-6c46f87778-7s8vj
d50f1f5bd254b       07655ddf2eebe                                                                                                                6 hours ago         Running             kubernetes-dashboard        8                   ab9076a6b93fe       kubernetes-dashboard-8694d4445c-rxvk7
ccb6bce13eddd       6e38f40d628db                                                                                                                6 hours ago         Running             storage-provisioner         10                  720c407a70463       storage-provisioner
fce03fb13ee86       07655ddf2eebe                                                                                                                13 hours ago        Exited              kubernetes-dashboard        7                   ab9076a6b93fe       kubernetes-dashboard-8694d4445c-rxvk7
affbabedbdac1       6e38f40d628db                                                                                                                13 hours ago        Exited              storage-provisioner         9                   720c407a70463       storage-provisioner
71f5670db011c       nginx@sha256:84c52dfd55c467e12ef85cad6a252c0990564f03c4850799bf41dd738738691f                                                13 hours ago        Running             nginx                       3                   7a7fb9ad61af2       my-nginx-7fbf685c4d-tsq5b
854623456e4b5       5aa0bf4798fa2                                                                                                                13 hours ago        Running             controller                  3                   429f591fb7aa4       ingress-nginx-controller-7c6974c4d8-9wzk7
a33536d07cf13       ead0a4a53df89                                                                                                                13 hours ago        Running             coredns                     3                   bb8fd63d12966       coredns-5dd5756b68-tzbkr
2cf508a54d752       115053965e86b                                                                                                                13 hours ago        Running             dashboard-metrics-scraper   3                   84fa73ebbe519       dashboard-metrics-scraper-7fd5cb4ddc-ckjl7
73ccbf113d322       bfc896cf80fba                                                                                                                13 hours ago        Running             kube-proxy                  3                   617b37a7b86c1       kube-proxy-qt5dx
1f69dba09634c       6d1b4fd1b182d                                                                                                                13 hours ago        Running             kube-scheduler              3                   30acf1c130528       kube-scheduler-minikube
859aacedb2b28       10baa1ca17068                                                                                                                13 hours ago        Running             kube-controller-manager     3                   4131805a1ad8e       kube-controller-manager-minikube
c7921debdec52       73deb9a3f7025                                                                                                                13 hours ago        Running             etcd                        3                   db2c9d9ce143c       etcd-minikube
a56082fbeafbc       5374347291230                                                                                                                13 hours ago        Running             kube-apiserver              3                   f4437c1c2c3d8       kube-apiserver-minikube
0386d4a215bb7       nginx@sha256:84c52dfd55c467e12ef85cad6a252c0990564f03c4850799bf41dd738738691f                                                27 hours ago        Exited              nginx                       2                   dd1eaebddc8be       my-nginx-7fbf685c4d-tsq5b
4db683e9cbba1       5aa0bf4798fa2                                                                                                                27 hours ago        Exited              controller                  2                   cd06d0a3ea33d       ingress-nginx-controller-7c6974c4d8-9wzk7
6dcafa9f13fca       ead0a4a53df89                                                                                                                27 hours ago        Exited              coredns                     2                   2a50946d338c8       coredns-5dd5756b68-tzbkr
1cd80f8091a1e       115053965e86b                                                                                                                27 hours ago        Exited              dashboard-metrics-scraper   2                   517122942ee86       dashboard-metrics-scraper-7fd5cb4ddc-ckjl7
746228c9c10ea       bfc896cf80fba                                                                                                                27 hours ago        Exited              kube-proxy                  2                   ea97b06cc5da4       kube-proxy-qt5dx
0b08d1069232b       5374347291230                                                                                                                27 hours ago        Exited              kube-apiserver              2                   4aac1280ed2d6       kube-apiserver-minikube
37744438c907a       73deb9a3f7025                                                                                                                27 hours ago        Exited              etcd                        2                   3725212f51ba4       etcd-minikube
98517ad6c1994       10baa1ca17068                                                                                                                27 hours ago        Exited              kube-controller-manager     2                   ecb68b099a5b5       kube-controller-manager-minikube
28547570b9d46       6d1b4fd1b182d                                                                                                                27 hours ago        Exited              kube-scheduler              2                   4191535c336f3       kube-scheduler-minikube
49d4cf8518e36       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80   38 hours ago        Exited              patch                       0                   eee42d3c602f9       ingress-nginx-admission-patch-5rxcc
1b47d0fc1d7ba       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80   38 hours ago        Exited              create                      0                   e78ba50caed8b       ingress-nginx-admission-create-4t64b

* 
* ==> controller_ingress [4db683e9cbba] <==
* W0207 10:01:01.303374       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0207 10:01:01.303715       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0207 10:01:01.414568       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0207 10:01:02.011909       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0207 10:01:02.106406       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0207 10:01:02.124747       7 nginx.go:260] "Starting NGINX Ingress controller"
I0207 10:01:02.140061       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"d0d7786f-1acc-4066-b61b-a34d3db129d2", APIVersion:"v1", ResourceVersion:"456", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0207 10:01:02.141873       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"5598ae79-cdfd-4997-99ce-6d84257b7cf6", APIVersion:"v1", ResourceVersion:"457", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0207 10:01:02.141925       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"c816b960-9595-4d61-a95f-68f2f4be5cc1", APIVersion:"v1", ResourceVersion:"458", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0207 10:01:03.328508       7 nginx.go:303] "Starting NGINX process"
I0207 10:01:03.328576       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0207 10:01:03.328927       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0207 10:01:03.330299       7 controller.go:190] "Configuration changes detected, backend reload required"
I0207 10:01:03.338034       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0207 10:01:03.339333       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-9wzk7"
I0207 10:01:03.343476       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-9wzk7" node="minikube"
I0207 10:01:03.504705       7 controller.go:210] "Backend successfully reloaded"
I0207 10:01:03.504989       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0207 10:01:03.505177       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-9wzk7", UID:"c79fc6e0-ca63-41ab-b223-9dc06f8a7ef9", APIVersion:"v1", ResourceVersion:"20206", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
E0207 10:36:06.531464       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0207 10:36:06.626835       7 leaderelection.go:280] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0207 10:36:06.647618       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0207 10:36:06.651118       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0207 10:36:07.240959       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0207 10:36:07.450081       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-9wzk7" node="minikube"
E0207 17:48:16.210351       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: client rate limiter Wait returned an error: context deadline exceeded
I0207 17:48:16.214363       7 leaderelection.go:280] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
I0207 17:48:16.221844       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
W0207 17:48:16.229213       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:16.282929       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:16.285158       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:16.285432       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:16.296276       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0207 17:48:16.296334       7 status.go:104] "error running poll" err="timed out waiting for the condition"
W0207 17:48:16.296523       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0207 17:48:17.997539       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0207 19:50:14.736535       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-9wzk7" node="minikube"
E0207 21:34:22.915037       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0207 21:34:23.422591       7 leaderelection.go:280] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0207 21:34:23.490962       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0207 21:34:23.687443       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0207 21:34:41.783765       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0207 21:43:41.650828       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-9wzk7" node="minikube"
I0207 23:44:15.351429       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0207 23:44:15.353018       7 nginx.go:379] "Shutting down controller queues"
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------


* 
* ==> controller_ingress [854623456e4b] <==
* W0207 23:51:35.175244       7 main.go:246] Initial connection to the Kubernetes API server was retried 1 times.
I0207 23:51:35.175312       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0207 23:51:35.567099       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0207 23:51:35.593621       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0207 23:51:35.608832       7 nginx.go:260] "Starting NGINX Ingress controller"
I0207 23:51:35.616201       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"d0d7786f-1acc-4066-b61b-a34d3db129d2", APIVersion:"v1", ResourceVersion:"456", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0207 23:51:35.620652       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"5598ae79-cdfd-4997-99ce-6d84257b7cf6", APIVersion:"v1", ResourceVersion:"457", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0207 23:51:35.620806       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"c816b960-9595-4d61-a95f-68f2f4be5cc1", APIVersion:"v1", ResourceVersion:"458", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0207 23:51:36.812010       7 nginx.go:303] "Starting NGINX process"
I0207 23:51:36.812262       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0207 23:51:36.812601       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0207 23:51:36.814180       7 controller.go:190] "Configuration changes detected, backend reload required"
I0207 23:51:36.819409       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0207 23:51:36.819529       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-9wzk7"
I0207 23:51:36.827957       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-9wzk7" node="minikube"
I0207 23:51:36.947818       7 controller.go:210] "Backend successfully reloaded"
I0207 23:51:36.947904       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0207 23:51:36.947969       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-9wzk7", UID:"c79fc6e0-ca63-41ab-b223-9dc06f8a7ef9", APIVersion:"v1", ResourceVersion:"55820", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0208 06:36:30.061407       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0208 06:36:30.153609       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.IngressClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0208 06:36:30.153717       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Secret ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0208 06:36:30.153780       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0208 06:36:30.154581       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0208 06:36:30.273968       7 reflector.go:456] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: watch of *v1.Ingress ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0208 06:36:40.778259       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": net/http: TLS handshake timeout
W0208 06:36:41.367351       7 reflector.go:533] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=66087": net/http: TLS handshake timeout
W0208 06:36:41.368483       7 reflector.go:533] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.IngressClass: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=66087": net/http: TLS handshake timeout
I0208 06:36:41.469859       7 trace.go:219] Trace[1939817719]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Feb-2024 06:36:31.170) (total time: 10277ms):
Trace[1939817719]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=66087": net/http: TLS handshake timeout 10186ms (06:36:41.356)
Trace[1939817719]: [10.277052859s] [10.277052859s] END
I0208 06:36:41.562870       7 trace.go:219] Trace[1475987385]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Feb-2024 06:36:31.077) (total time: 10482ms):
Trace[1475987385]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=66087": net/http: TLS handshake timeout 10270ms (06:36:41.348)
Trace[1475987385]: [10.482076761s] [10.482076761s] END
W0208 06:36:41.680621       7 reflector.go:533] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=66087": net/http: TLS handshake timeout
W0208 06:36:41.688234       7 reflector.go:533] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Ingress: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingresses?resourceVersion=66087": net/http: TLS handshake timeout
I0208 06:36:41.688840       7 trace.go:219] Trace[1197733062]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Feb-2024 06:36:31.347) (total time: 10341ms):
Trace[1197733062]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingresses?resourceVersion=66087": net/http: TLS handshake timeout 10340ms (06:36:41.688)
Trace[1197733062]: [10.341196631s] [10.341196631s] END
I0208 06:36:41.692175       7 trace.go:219] Trace[71757236]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Feb-2024 06:36:31.591) (total time: 10089ms):
Trace[71757236]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=66087": net/http: TLS handshake timeout 10089ms (06:36:41.680)
Trace[71757236]: [10.089633198s] [10.089633198s] END
E0208 06:36:41.692536       7 reflector.go:148] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=66087": net/http: TLS handshake timeout
E0208 06:36:41.749430       7 reflector.go:148] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=66087": net/http: TLS handshake timeout
E0208 06:36:41.764599       7 reflector.go:148] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Ingress: failed to list *v1.Ingress: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingresses?resourceVersion=66087": net/http: TLS handshake timeout
W0208 06:36:41.767035       7 reflector.go:533] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Secret: Get "https://10.96.0.1:443/api/v1/secrets?fieldSelector=%!C(MISSING)type%3Dhelm.sh%!F(MISSING)release.v1&resourceVersion=66087": net/http: TLS handshake timeout
I0208 06:36:41.767429       7 trace.go:219] Trace[1309336391]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Feb-2024 06:36:31.641) (total time: 10126ms):
Trace[1309336391]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/secrets?fieldSelector=%!C(MISSING)type%3Dhelm.sh%!F(MISSING)release.v1&resourceVersion=66087": net/http: TLS handshake timeout 10125ms (06:36:41.766)
Trace[1309336391]: [10.126103094s] [10.126103094s] END
E0208 06:36:41.767570       7 reflector.go:148] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Secret: failed to list *v1.Secret: Get "https://10.96.0.1:443/api/v1/secrets?fieldSelector=%!C(MISSING)type%3Dhelm.sh%!F(MISSING)release.v1&resourceVersion=66087": net/http: TLS handshake timeout
E0208 06:36:41.856743       7 queue.go:131] "requeuing" err="Get \"https://10.96.0.1:443/api/v1/namespaces/ingress-nginx/pods?labelSelector=app.kubernetes.io%!F(MISSING)component%!D(MISSING)controller%!C(MISSING)app.kubernetes.io%!F(MISSING)instance%!D(MISSING)ingress-nginx%!C(MISSING)app.kubernetes.io%!F(MISSING)name%!D(MISSING)ingress-nginx%!C(MISSING)gcp-auth-skip-secret%!D(MISSING)true%!C(MISSING)pod-template-hash%!D(MISSING)7c6974c4d8\": net/http: TLS handshake timeout" key="&ObjectMeta{Name:sync status,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{},Annotations:map[string]string{},OwnerReferences:[]OwnerReference{},Finalizers:[],ManagedFields:[]ManagedFieldsEntry{},}"
W0208 06:36:41.868526       7 reflector.go:533] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.ConfigMap: Get "https://10.96.0.1:443/api/v1/configmaps?labelSelector=OWNER%3DTILLER&resourceVersion=66087": net/http: TLS handshake timeout
I0208 06:36:41.868865       7 trace.go:219] Trace[232824216]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (08-Feb-2024 06:36:31.163) (total time: 10705ms):
Trace[232824216]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/configmaps?labelSelector=OWNER%3DTILLER&resourceVersion=66087": net/http: TLS handshake timeout 10705ms (06:36:41.868)
Trace[232824216]: [10.705524232s] [10.705524232s] END
E0208 06:36:41.869080       7 reflector.go:148] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://10.96.0.1:443/api/v1/configmaps?labelSelector=OWNER%3DTILLER&resourceVersion=66087": net/http: TLS handshake timeout
E0208 06:36:41.749575       7 reflector.go:148] k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.IngressClass: failed to list *v1.IngressClass: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=66087": net/http: TLS handshake timeout
I0208 06:36:44.650937       7 leaderelection.go:280] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0208 06:36:44.947418       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0208 06:36:44.959937       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0208 06:36:52.646201       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader

* 
* ==> coredns [6dcafa9f13fc] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:42918 - 56664 "HINFO IN 6039049888148324629.7535729808145154203. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.117114804s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.362931575s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.17581421s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.588228509s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.306439468s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.527160647s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.478511248s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.460745621s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.013369581s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 4.451722447s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.445545407s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [a33536d07cf1] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:50170 - 35479 "HINFO IN 5508295675584509864.2499540520855200541. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.309415305s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.176061821s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.001002103s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.218540274s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_02_06T22_38_31_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 06 Feb 2024 22:38:28 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 08 Feb 2024 12:52:37 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 08 Feb 2024 12:50:26 +0000   Thu, 08 Feb 2024 12:50:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 08 Feb 2024 12:50:26 +0000   Thu, 08 Feb 2024 12:50:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 08 Feb 2024 12:50:26 +0000   Thu, 08 Feb 2024 12:50:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 08 Feb 2024 12:50:26 +0000   Thu, 08 Feb 2024 12:50:26 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8150488Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8150488Ki
  pods:               110
System Info:
  Machine ID:                 5bd4126768034933bae04f0cc6bfdf86
  System UUID:                6144fc94-d201-42dc-a821-8f6fd4792cc6
  Boot ID:                    56e2528b-cd6c-4303-a9c6-8ee3c9e9685c
  Kernel Version:             5.15.49-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     my-nginx-7fbf685c4d-tsq5b                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  default                     productsupportbackendapp-6c46f87778-7s8vj     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27s
  ingress-nginx               ingress-nginx-controller-7c6974c4d8-9wzk7     100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         38h
  kube-system                 coredns-5dd5756b68-tzbkr                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     38h
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         38h
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 kube-proxy-qt5dx                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-ckjl7    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-rxvk7         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         38h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (3%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  NodeNotReady             2m15s (x4 over 6h6m)   node-controller  Node minikube status is now: NodeNotReady
  Normal  NodeHasSufficientMemory  2m13s (x33 over 13h)   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m13s (x33 over 13h)   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m13s (x33 over 13h)   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeReady                2m13s (x5 over 7h56m)  kubelet          Node minikube status is now: NodeReady

* 
* ==> dmesg <==
* [  +0.000507]  ? task_group_account_field+0x45/0x4f
[  +0.004525]  ? tick_sched_do_timer+0x3e/0x3e
[  +0.000045]  update_process_times+0x8c/0xab
[  +0.000990]  tick_sched_handle+0x48/0x54
[  +0.000467]  tick_sched_timer+0x38/0x65
[  +0.000006]  __hrtimer_run_queues+0xf9/0x18b
[  +0.000004]  hrtimer_interrupt+0x92/0x160
[  +0.000003]  __sysvec_apic_timer_interrupt+0x96/0xdb
[  +0.000560]  sysvec_apic_timer_interrupt+0x37/0x7d
[  +0.002471]  ? asm_sysvec_apic_timer_interrupt+0xa/0x20
[  +0.000042]  asm_sysvec_apic_timer_interrupt+0x12/0x20
[  +0.002047] RIP: 0033:0x488720
[  +0.000765] Code: 08 48 8b 5c 24 10 48 8b 4c 24 18 e9 3a ff ff ff cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc <49> 3b 66 10 0f 86 9b 00 00 00 48 83 ec 30 48 89 6c 24 28 48 8d 6c
[  +0.000002] RSP: 002b:000000c001e19b20 EFLAGS: 00000246
[  +0.000002] RAX: 000000000000000f RBX: 000000c001bcc6b8 RCX: 000000c001e19b70
[  +0.000001] RDX: 0000000000000000 RSI: 000000c001aa6900 RDI: 0000000000000000
[  +0.000001] RBP: 000000c001e19b88 R08: 00000000000000c8 R09: 000000c001bcc600
[  +0.000012] R10: 000000006000de63 R11: faaaa8000000802a R12: ffffffffffffffff
[  +0.000001] R13: 0000000000000000 R14: 000000c0015644e0 R15: 0000000000010000
[  +0.000016]  </TASK>
[Feb 8 05:34] Hangcheck: hangcheck value past margin!
[  +0.001602] INFO: task jbd2/vda1-8:581 blocked for more than 999 seconds.
[  +0.002714]       Tainted: G           O    T 5.15.49-linuxkit #1
[  +0.004155] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Feb 8 05:36] INFO: task jbd2/vda1-8:581 blocked for more than 1122 seconds.
[  +0.003123]       Tainted: G           O    T 5.15.49-linuxkit #1
[  +0.002786] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Feb 8 05:52] INFO: task jbd2/vda1-8:581 blocked for more than 2068 seconds.
[  +0.002606]       Tainted: G           O    T 5.15.49-linuxkit #1
[  +0.000661] Hangcheck: hangcheck value past margin!
[  +0.004935] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Feb 8 05:54] INFO: task jbd2/vda1-8:581 blocked for more than 2191 seconds.
[  +0.002613]       Tainted: G           O    T 5.15.49-linuxkit #1
[  +0.002270] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Feb 8 06:10] INFO: task jbd2/vda1-8:581 blocked for more than 3149 seconds.
[  +0.003364]       Tainted: G           O    T 5.15.49-linuxkit #1
[  +0.000154] Hangcheck: hangcheck value past margin!
[  +0.001885] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Feb 8 06:18] Hangcheck: hangcheck value past margin!
[  +0.003211] INFO: task jbd2/vda1-8:581 blocked for more than 3668 seconds.
[  +0.002199]       Tainted: G           O    T 5.15.49-linuxkit #1
[Feb 8 06:19] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Feb 8 06:35] Hangcheck: hangcheck value past margin!
[  +0.001810] INFO: task jbd2/vda1-8:581 blocked for more than 4663 seconds.
[  +0.004837]       Tainted: G           O    T 5.15.49-linuxkit #1
[  +0.002389] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
[Feb 8 06:36] systemd-journald[235613]: File /run/log/journal/5bd4126768034933bae04f0cc6bfdf86/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Feb 8 06:45] Hangcheck: hangcheck value past margin!
[  +1.385928] systemd-journald[237412]: File /run/log/journal/5bd4126768034933bae04f0cc6bfdf86/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Feb 8 06:59] Hangcheck: hangcheck value past margin!
[Feb 8 07:16] Hangcheck: hangcheck value past margin!
[Feb 8 07:33] Hangcheck: hangcheck value past margin!
[Feb 8 07:54] Hangcheck: hangcheck value past margin!
[Feb 8 08:00] Hangcheck: hangcheck value past margin!
[Feb 8 08:21] Hangcheck: hangcheck value past margin!
[Feb 8 08:33] Hangcheck: hangcheck value past margin!
[Feb 8 11:17] Hangcheck: hangcheck value past margin!
[  +2.618030] systemd-journald[429557]: File /run/log/journal/5bd4126768034933bae04f0cc6bfdf86/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Feb 8 12:50] Hangcheck: hangcheck value past margin!
[  +2.897610] systemd-journald[545227]: File /run/log/journal/5bd4126768034933bae04f0cc6bfdf86/system.journal corrupted or uncleanly shut down, renaming and replacing.

* 
* ==> etcd [37744438c907] <==
* {"level":"info","ts":"2024-02-07T22:28:39.193355Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51011}
{"level":"info","ts":"2024-02-07T22:28:39.194674Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":51011,"took":"768.401¬µs","hash":3688459540}
{"level":"info","ts":"2024-02-07T22:28:39.194729Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3688459540,"revision":51011,"compact-revision":50631}
{"level":"info","ts":"2024-02-07T22:33:02.603109Z","caller":"traceutil/trace.go:171","msg":"trace[1873490041] transaction","detail":"{read_only:false; response_revision:51536; number_of_response:1; }","duration":"115.630717ms","start":"2024-02-07T22:33:02.487466Z","end":"2024-02-07T22:33:02.603096Z","steps":["trace[1873490041] 'process raft request'  (duration: 115.526182ms)"],"step_count":1}
{"level":"info","ts":"2024-02-07T22:33:38.988476Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51290}
{"level":"info","ts":"2024-02-07T22:33:38.995015Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":51290,"took":"5.282428ms","hash":2252531276}
{"level":"info","ts":"2024-02-07T22:33:38.995117Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2252531276,"revision":51290,"compact-revision":51011}
{"level":"info","ts":"2024-02-07T22:38:38.72353Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51571}
{"level":"info","ts":"2024-02-07T22:38:38.729599Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":51571,"took":"5.693765ms","hash":3093595847}
{"level":"info","ts":"2024-02-07T22:38:38.729718Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3093595847,"revision":51571,"compact-revision":51290}
{"level":"info","ts":"2024-02-07T22:43:38.512156Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51897}
{"level":"info","ts":"2024-02-07T22:43:38.513063Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":51897,"took":"737.664¬µs","hash":2489529633}
{"level":"info","ts":"2024-02-07T22:43:38.513107Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2489529633,"revision":51897,"compact-revision":51571}
{"level":"info","ts":"2024-02-07T22:48:38.301074Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52183}
{"level":"info","ts":"2024-02-07T22:48:38.306135Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":52183,"took":"4.906328ms","hash":1291098899}
{"level":"info","ts":"2024-02-07T22:48:38.306194Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1291098899,"revision":52183,"compact-revision":51897}
{"level":"info","ts":"2024-02-07T22:53:38.135095Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52512}
{"level":"info","ts":"2024-02-07T22:53:38.137377Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":52512,"took":"1.727586ms","hash":4203391563}
{"level":"info","ts":"2024-02-07T22:53:38.137451Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4203391563,"revision":52512,"compact-revision":52183}
{"level":"info","ts":"2024-02-07T22:58:37.925836Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52791}
{"level":"info","ts":"2024-02-07T22:58:37.927035Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":52791,"took":"1.0401ms","hash":986588501}
{"level":"info","ts":"2024-02-07T22:58:37.927077Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":986588501,"revision":52791,"compact-revision":52512}
{"level":"info","ts":"2024-02-07T23:03:37.760452Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53073}
{"level":"info","ts":"2024-02-07T23:03:37.767862Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":53073,"took":"6.302663ms","hash":4127196696}
{"level":"info","ts":"2024-02-07T23:03:37.767961Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4127196696,"revision":53073,"compact-revision":52791}
{"level":"info","ts":"2024-02-07T23:08:37.551549Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53352}
{"level":"info","ts":"2024-02-07T23:08:37.552499Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":53352,"took":"635.559¬µs","hash":956700309}
{"level":"info","ts":"2024-02-07T23:08:37.552519Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":956700309,"revision":53352,"compact-revision":53073}
{"level":"info","ts":"2024-02-07T23:13:37.342097Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53632}
{"level":"info","ts":"2024-02-07T23:13:37.342721Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":53632,"took":"472.87¬µs","hash":3578948368}
{"level":"info","ts":"2024-02-07T23:13:37.34276Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3578948368,"revision":53632,"compact-revision":53352}
{"level":"info","ts":"2024-02-07T23:18:37.132822Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53912}
{"level":"info","ts":"2024-02-07T23:18:37.133543Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":53912,"took":"550.584¬µs","hash":958742970}
{"level":"info","ts":"2024-02-07T23:18:37.133585Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":958742970,"revision":53912,"compact-revision":53632}
{"level":"info","ts":"2024-02-07T23:21:35.373755Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000001-00000000000100b7.wal"}
{"level":"info","ts":"2024-02-07T23:23:36.904444Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54192}
{"level":"info","ts":"2024-02-07T23:23:36.905959Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":54192,"took":"1.264598ms","hash":4228404465}
{"level":"info","ts":"2024-02-07T23:23:36.906008Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4228404465,"revision":54192,"compact-revision":53912}
{"level":"info","ts":"2024-02-07T23:28:36.696268Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54474}
{"level":"info","ts":"2024-02-07T23:28:36.697214Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":54474,"took":"756.018¬µs","hash":2834431148}
{"level":"info","ts":"2024-02-07T23:28:36.697265Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2834431148,"revision":54474,"compact-revision":54192}
{"level":"info","ts":"2024-02-07T23:33:36.490002Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54753}
{"level":"info","ts":"2024-02-07T23:33:36.491413Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":54753,"took":"1.063327ms","hash":1443928705}
{"level":"info","ts":"2024-02-07T23:33:36.491484Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1443928705,"revision":54753,"compact-revision":54474}
{"level":"info","ts":"2024-02-07T23:38:36.285295Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55033}
{"level":"info","ts":"2024-02-07T23:38:36.286558Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":55033,"took":"755.02¬µs","hash":286293617}
{"level":"info","ts":"2024-02-07T23:38:36.28668Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":286293617,"revision":55033,"compact-revision":54753}
{"level":"info","ts":"2024-02-07T23:43:36.075779Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55315}
{"level":"info","ts":"2024-02-07T23:43:36.077197Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":55315,"took":"1.068941ms","hash":4260396669}
{"level":"info","ts":"2024-02-07T23:43:36.077256Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4260396669,"revision":55315,"compact-revision":55033}
{"level":"info","ts":"2024-02-07T23:44:14.18296Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-02-07T23:44:14.289945Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-02-07T23:44:14.589676Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-07T23:44:14.64763Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-07T23:44:15.557446Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-07T23:44:15.557592Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-02-07T23:44:15.631121Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-07T23:44:15.945164Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-07T23:44:15.959093Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-07T23:44:15.960412Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [c7921debdec5] <==
* {"level":"info","ts":"2024-02-08T11:52:44.494405Z","caller":"traceutil/trace.go:171","msg":"trace[314512271] transaction","detail":"{read_only:false; response_revision:76632; number_of_response:1; }","duration":"144.774764ms","start":"2024-02-08T11:52:44.349604Z","end":"2024-02-08T11:52:44.494379Z","steps":["trace[314512271] 'process raft request'  (duration: 144.64682ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T11:57:31.688858Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":76620}
{"level":"info","ts":"2024-02-08T11:57:31.690458Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":76620,"took":"1.372706ms","hash":1251705871}
{"level":"info","ts":"2024-02-08T11:57:31.690494Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1251705871,"revision":76620,"compact-revision":76341}
{"level":"info","ts":"2024-02-08T12:02:31.482918Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":76902}
{"level":"info","ts":"2024-02-08T12:02:31.484341Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":76902,"took":"927.893¬µs","hash":2471621935}
{"level":"info","ts":"2024-02-08T12:02:31.484412Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2471621935,"revision":76902,"compact-revision":76620}
{"level":"info","ts":"2024-02-08T12:07:31.281883Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":77181}
{"level":"info","ts":"2024-02-08T12:07:31.283039Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":77181,"took":"856.309¬µs","hash":139693748}
{"level":"info","ts":"2024-02-08T12:07:31.283108Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":139693748,"revision":77181,"compact-revision":76902}
{"level":"info","ts":"2024-02-08T12:12:31.078995Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":77460}
{"level":"info","ts":"2024-02-08T12:12:31.079994Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":77460,"took":"699.597¬µs","hash":1723817777}
{"level":"info","ts":"2024-02-08T12:12:31.080081Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1723817777,"revision":77460,"compact-revision":77181}
{"level":"info","ts":"2024-02-08T12:17:30.874677Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":77741}
{"level":"info","ts":"2024-02-08T12:17:30.875518Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":77741,"took":"668.596¬µs","hash":550910549}
{"level":"info","ts":"2024-02-08T12:17:30.87557Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":550910549,"revision":77741,"compact-revision":77460}
{"level":"info","ts":"2024-02-08T12:22:30.602813Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":78021}
{"level":"info","ts":"2024-02-08T12:22:30.604597Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":78021,"took":"1.407449ms","hash":3217218087}
{"level":"info","ts":"2024-02-08T12:22:30.604666Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3217218087,"revision":78021,"compact-revision":77741}
{"level":"info","ts":"2024-02-08T12:25:57.102807Z","caller":"traceutil/trace.go:171","msg":"trace[2033098052] transaction","detail":"{read_only:false; response_revision:78544; number_of_response:1; }","duration":"631.538094ms","start":"2024-02-08T12:25:56.471256Z","end":"2024-02-08T12:25:57.102794Z","steps":["trace[2033098052] 'process raft request'  (duration: 631.288895ms)"],"step_count":1}
{"level":"warn","ts":"2024-02-08T12:25:57.103187Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"207.755912ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-02-08T12:25:57.10329Z","caller":"traceutil/trace.go:171","msg":"trace[1142917543] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:78544; }","duration":"207.858548ms","start":"2024-02-08T12:25:56.895422Z","end":"2024-02-08T12:25:57.10328Z","steps":["trace[1142917543] 'agreement among raft nodes before linearized reading'  (duration: 207.727986ms)"],"step_count":1}
{"level":"warn","ts":"2024-02-08T12:25:57.103883Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-02-08T12:25:56.471239Z","time spent":"631.698188ms","remote":"127.0.0.1:40162","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:78543 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-02-08T12:25:57.103974Z","caller":"traceutil/trace.go:171","msg":"trace[1377021968] linearizableReadLoop","detail":"{readStateIndex:94999; appliedIndex:94998; }","duration":"207.280787ms","start":"2024-02-08T12:25:56.895446Z","end":"2024-02-08T12:25:57.102727Z","steps":["trace[1377021968] 'read index received'  (duration: 207.047196ms)","trace[1377021968] 'applied index is now lower than readState.Index'  (duration: 232.825¬µs)"],"step_count":2}
{"level":"info","ts":"2024-02-08T12:27:30.392649Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":78303}
{"level":"info","ts":"2024-02-08T12:27:30.402893Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":78303,"took":"9.228492ms","hash":429796160}
{"level":"info","ts":"2024-02-08T12:27:30.402954Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":429796160,"revision":78303,"compact-revision":78021}
{"level":"info","ts":"2024-02-08T12:32:30.186695Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":78632}
{"level":"info","ts":"2024-02-08T12:32:30.201262Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":78632,"took":"13.391113ms","hash":3537071792}
{"level":"info","ts":"2024-02-08T12:32:30.20137Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3537071792,"revision":78632,"compact-revision":78303}
{"level":"info","ts":"2024-02-08T12:37:29.965335Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":78911}
{"level":"info","ts":"2024-02-08T12:37:29.966027Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":78911,"took":"583.477¬µs","hash":2511281138}
{"level":"info","ts":"2024-02-08T12:37:29.966045Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2511281138,"revision":78911,"compact-revision":78632}
{"level":"info","ts":"2024-02-08T12:42:29.751398Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":79193}
{"level":"info","ts":"2024-02-08T12:42:29.752109Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":79193,"took":"496.957¬µs","hash":3492865722}
{"level":"info","ts":"2024-02-08T12:42:29.752251Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3492865722,"revision":79193,"compact-revision":78911}
{"level":"info","ts":"2024-02-08T12:50:23.866644Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":79472}
{"level":"info","ts":"2024-02-08T12:50:23.86837Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":79472,"took":"1.514453ms","hash":2917024544}
{"level":"info","ts":"2024-02-08T12:50:23.868416Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2917024544,"revision":79472,"compact-revision":79193}
{"level":"info","ts":"2024-02-08T12:50:24.782321Z","caller":"traceutil/trace.go:171","msg":"trace[533584799] linearizableReadLoop","detail":"{readStateIndex:96306; appliedIndex:96302; }","duration":"104.139501ms","start":"2024-02-08T12:50:24.677465Z","end":"2024-02-08T12:50:24.781605Z","steps":["trace[533584799] 'read index received'  (duration: 101.583693ms)","trace[533584799] 'applied index is now lower than readState.Index'  (duration: 2.554895ms)"],"step_count":2}
{"level":"warn","ts":"2024-02-08T12:50:24.78316Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.685267ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/my-nginx-7fbf685c4d-tsq5b.17b1d04dd0c4ebf4\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-02-08T12:50:24.785935Z","caller":"traceutil/trace.go:171","msg":"trace[119628310] range","detail":"{range_begin:/registry/events/default/my-nginx-7fbf685c4d-tsq5b.17b1d04dd0c4ebf4; range_end:; response_count:0; response_revision:79621; }","duration":"108.465899ms","start":"2024-02-08T12:50:24.677448Z","end":"2024-02-08T12:50:24.785914Z","steps":["trace[119628310] 'agreement among raft nodes before linearized reading'  (duration: 105.619316ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:24.78382Z","caller":"traceutil/trace.go:171","msg":"trace[807604254] transaction","detail":"{read_only:false; response_revision:79617; number_of_response:1; }","duration":"110.04523ms","start":"2024-02-08T12:50:24.673756Z","end":"2024-02-08T12:50:24.783801Z","steps":["trace[807604254] 'process raft request'  (duration: 105.270424ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:24.78394Z","caller":"traceutil/trace.go:171","msg":"trace[2054266048] transaction","detail":"{read_only:false; response_revision:79618; number_of_response:1; }","duration":"107.012099ms","start":"2024-02-08T12:50:24.676918Z","end":"2024-02-08T12:50:24.78393Z","steps":["trace[2054266048] 'process raft request'  (duration: 104.386944ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:24.784021Z","caller":"traceutil/trace.go:171","msg":"trace[1339479192] transaction","detail":"{read_only:false; response_revision:79620; number_of_response:1; }","duration":"106.600394ms","start":"2024-02-08T12:50:24.677411Z","end":"2024-02-08T12:50:24.784011Z","steps":["trace[1339479192] 'process raft request'  (duration: 104.095277ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:24.784076Z","caller":"traceutil/trace.go:171","msg":"trace[1006713690] transaction","detail":"{read_only:false; response_revision:79619; number_of_response:1; }","duration":"106.920883ms","start":"2024-02-08T12:50:24.677147Z","end":"2024-02-08T12:50:24.784068Z","steps":["trace[1006713690] 'process raft request'  (duration: 104.311537ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:24.784517Z","caller":"traceutil/trace.go:171","msg":"trace[1778168043] transaction","detail":"{read_only:false; response_revision:79621; number_of_response:1; }","duration":"101.222629ms","start":"2024-02-08T12:50:24.683285Z","end":"2024-02-08T12:50:24.784507Z","steps":["trace[1778168043] 'process raft request'  (duration: 98.256168ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:25.106532Z","caller":"traceutil/trace.go:171","msg":"trace[187686671] transaction","detail":"{read_only:false; response_revision:79628; number_of_response:1; }","duration":"114.116515ms","start":"2024-02-08T12:50:24.992393Z","end":"2024-02-08T12:50:25.106509Z","steps":["trace[187686671] 'process raft request'  (duration: 113.996767ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:25.374355Z","caller":"traceutil/trace.go:171","msg":"trace[2075982740] transaction","detail":"{read_only:false; response_revision:79629; number_of_response:1; }","duration":"213.471861ms","start":"2024-02-08T12:50:25.160856Z","end":"2024-02-08T12:50:25.374328Z","steps":["trace[2075982740] 'process raft request'  (duration: 200.051898ms)","trace[2075982740] 'compare'  (duration: 12.61061ms)"],"step_count":2}
{"level":"warn","ts":"2024-02-08T12:50:25.582584Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.138304ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128027034082397657 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/daemonsets/kube-system/kube-proxy\" mod_revision:74628 > success:<request_put:<key:\"/registry/daemonsets/kube-system/kube-proxy\" value_size:2831 >> failure:<request_range:<key:\"/registry/daemonsets/kube-system/kube-proxy\" > >>","response":"size:18"}
{"level":"info","ts":"2024-02-08T12:50:25.591863Z","caller":"traceutil/trace.go:171","msg":"trace[1380430116] transaction","detail":"{read_only:false; response_revision:79631; number_of_response:1; }","duration":"217.839092ms","start":"2024-02-08T12:50:25.373985Z","end":"2024-02-08T12:50:25.591824Z","steps":["trace[1380430116] 'process raft request'  (duration: 107.397367ms)","trace[1380430116] 'compare'  (duration: 101.05748ms)"],"step_count":2}
{"level":"info","ts":"2024-02-08T12:50:25.714479Z","caller":"traceutil/trace.go:171","msg":"trace[836544902] linearizableReadLoop","detail":"{readStateIndex:96320; appliedIndex:96316; }","duration":"214.807679ms","start":"2024-02-08T12:50:25.499653Z","end":"2024-02-08T12:50:25.71446Z","steps":["trace[836544902] 'read index received'  (duration: 68.33147ms)","trace[836544902] 'applied index is now lower than readState.Index'  (duration: 146.475469ms)"],"step_count":2}
{"level":"warn","ts":"2024-02-08T12:50:25.71469Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"215.062051ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-proxy-qt5dx.17b1d04da7d49769\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-02-08T12:50:25.714755Z","caller":"traceutil/trace.go:171","msg":"trace[768863273] range","detail":"{range_begin:/registry/events/kube-system/kube-proxy-qt5dx.17b1d04da7d49769; range_end:; response_count:0; response_revision:79635; }","duration":"215.140545ms","start":"2024-02-08T12:50:25.499601Z","end":"2024-02-08T12:50:25.714741Z","steps":["trace[768863273] 'agreement among raft nodes before linearized reading'  (duration: 214.969128ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:25.724939Z","caller":"traceutil/trace.go:171","msg":"trace[670287027] transaction","detail":"{read_only:false; response_revision:79634; number_of_response:1; }","duration":"237.779756ms","start":"2024-02-08T12:50:25.487132Z","end":"2024-02-08T12:50:25.724912Z","steps":["trace[670287027] 'process raft request'  (duration: 227.155164ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:50:25.72562Z","caller":"traceutil/trace.go:171","msg":"trace[1852613215] transaction","detail":"{read_only:false; response_revision:79633; number_of_response:1; }","duration":"238.710607ms","start":"2024-02-08T12:50:25.486833Z","end":"2024-02-08T12:50:25.725543Z","steps":["trace[1852613215] 'process raft request'  (duration: 200.536823ms)","trace[1852613215] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/replicasets/default/productsupportbackendapp-6c46f87778; req_size:2001; } (duration: 17.948472ms)"],"step_count":2}
{"level":"info","ts":"2024-02-08T12:50:25.783704Z","caller":"traceutil/trace.go:171","msg":"trace[483634578] transaction","detail":"{read_only:false; response_revision:79635; number_of_response:1; }","duration":"220.516805ms","start":"2024-02-08T12:50:25.56312Z","end":"2024-02-08T12:50:25.783637Z","steps":["trace[483634578] 'process raft request'  (duration: 151.273218ms)"],"step_count":1}
{"level":"info","ts":"2024-02-08T12:52:12.78817Z","caller":"traceutil/trace.go:171","msg":"trace[1782909177] transaction","detail":"{read_only:false; response_revision:79834; number_of_response:1; }","duration":"141.356208ms","start":"2024-02-08T12:52:12.646791Z","end":"2024-02-08T12:52:12.788147Z","steps":["trace[1782909177] 'process raft request'  (duration: 141.096877ms)"],"step_count":1}
{"level":"warn","ts":"2024-02-08T12:52:13.314153Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"255.629266ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:134"}
{"level":"info","ts":"2024-02-08T12:52:13.31432Z","caller":"traceutil/trace.go:171","msg":"trace[1231833211] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:79834; }","duration":"255.805621ms","start":"2024-02-08T12:52:13.058502Z","end":"2024-02-08T12:52:13.314307Z","steps":["trace[1231833211] 'range keys from in-memory index tree'  (duration: 255.477639ms)"],"step_count":1}

* 
* ==> kernel <==
*  12:52:40 up 13:08,  0 users,  load average: 1.85, 1.10, 0.63
Linux minikube 5.15.49-linuxkit #1 SMP Tue Sep 13 07:51:46 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [0b08d1069232] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0207 23:44:16.335839       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0207 23:44:16.335882       1 logging.go:59] [core] [Channel #1 SubChannel #3] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0207 23:44:16.335922       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0207 23:44:16.335961       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0207 23:44:16.336035       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0207 23:44:17.127918       1 logging.go:59] [core] [Channel #2 SubChannel #4] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0207 23:44:17.128070       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [a56082fbeafb] <==
* Trace[254516340]:  ---"About to Encode" 595ms (06:46:08.213)
Trace[254516340]:  ---"Txn call completed" 666ms (06:46:08.880)]
Trace[254516340]: [1.278658332s] [1.278658332s] END
I0208 06:46:08.882315       1 trace.go:236] Trace[1006088226]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1bdd4993-c7b3-412a-b5cc-5e9c4d48ce0e,client:192.168.49.2,protocol:HTTP/2.0,resource:deployments,scope:resource,url:/apis/apps/v1/namespaces/ingress-nginx/deployments/ingress-nginx-controller/status,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:deployment-controller,verb:PUT (08-Feb-2024 06:46:07.594) (total time: 1287ms):
Trace[1006088226]: ---"Conversion done" 18ms (06:46:07.613)
Trace[1006088226]: ["GuaranteedUpdate etcd3" audit-id:1bdd4993-c7b3-412a-b5cc-5e9c4d48ce0e,key:/deployments/ingress-nginx/ingress-nginx-controller,type:*apps.Deployment,resource:deployments.apps 1267ms (06:46:07.614)
Trace[1006088226]:  ---"About to Encode" 687ms (06:46:08.303)
Trace[1006088226]:  ---"Txn call completed" 574ms (06:46:08.881)]
Trace[1006088226]: [1.287356617s] [1.287356617s] END
I0208 06:46:18.420708       1 trace.go:236] Trace[1194495617]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b9c353de-f474-4c88-ab90-9f94a991764b,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:node-controller,verb:PATCH (08-Feb-2024 06:46:17.920) (total time: 500ms):
Trace[1194495617]: ---"About to check admission control" 282ms (06:46:18.206)
Trace[1194495617]: ---"Object stored in database" 210ms (06:46:18.416)
Trace[1194495617]: [500.27577ms] [500.27577ms] END
I0208 06:46:19.484716       1 trace.go:236] Trace[1758748835]: "Update" accept:application/json, */*,audit-id:d6de8231-898f-4166-85fa-89b49ea2c899,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Feb-2024 06:46:18.900) (total time: 582ms):
Trace[1758748835]: ---"limitedReadBody succeeded" len:1354 29ms (06:46:18.929)
Trace[1758748835]: ---"About to convert to expected version" 68ms (06:46:18.997)
Trace[1758748835]: ---"Writing http response done" 87ms (06:46:19.483)
Trace[1758748835]: [582.690165ms] [582.690165ms] END
I0208 06:46:35.144007       1 trace.go:236] Trace[617375145]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Feb-2024 06:46:34.018) (total time: 1124ms):
Trace[617375145]: ---"initial value restored" 324ms (06:46:34.343)
Trace[617375145]: ---"Transaction prepared" 504ms (06:46:34.847)
Trace[617375145]: ---"Txn call completed" 295ms (06:46:35.143)
Trace[617375145]: [1.124957156s] [1.124957156s] END
I0208 08:47:25.675107       1 trace.go:236] Trace[671717348]: "DeltaFIFO Pop Process" ID:v1.apps,Depth:19,Reason:slow event handlers blocking the queue (08-Feb-2024 08:47:25.346) (total time: 328ms):
Trace[671717348]: [328.198743ms] [328.198743ms] END
I0208 08:47:25.858148       1 trace.go:236] Trace[1608181008]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Feb-2024 08:47:24.773) (total time: 1084ms):
Trace[1608181008]: ---"initial value restored" 782ms (08:47:25.556)
Trace[1608181008]: ---"Transaction prepared" 200ms (08:47:25.757)
Trace[1608181008]: ---"Txn call completed" 100ms (08:47:25.857)
Trace[1608181008]: [1.084141899s] [1.084141899s] END
E0208 08:47:36.021059       1 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 3e271fdb-d4a3-452f-85bb-93ab263af764, UID in object meta: "
I0208 10:03:49.367029       1 trace.go:236] Trace[221310032]: "Get" accept:application/json, */*,audit-id:9316983b-c16f-4d4a-807f-001530c800db,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (08-Feb-2024 10:03:48.652) (total time: 712ms):
Trace[221310032]: ---"About to write a response" 712ms (10:03:49.364)
Trace[221310032]: [712.373797ms] [712.373797ms] END
I0208 11:17:32.969932       1 trace.go:236] Trace[1786174442]: "DeltaFIFO Pop Process" ID:v1.networking.k8s.io,Depth:19,Reason:slow event handlers blocking the queue (08-Feb-2024 11:17:32.404) (total time: 565ms):
Trace[1786174442]: [565.285403ms] [565.285403ms] END
I0208 11:17:33.281447       1 trace.go:236] Trace[701467169]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Feb-2024 11:17:32.404) (total time: 876ms):
Trace[701467169]: ---"initial value restored" 662ms (11:17:33.067)
Trace[701467169]: ---"Txn call completed" 167ms (11:17:33.281)
Trace[701467169]: [876.651126ms] [876.651126ms] END
I0208 12:25:34.283402       1 alloc.go:330] "allocated clusterIPs" service="default/productsupportbackendapp-service" clusterIPs={"IPv4":"10.104.237.102"}
I0208 12:25:57.105790       1 trace.go:236] Trace[34470207]: "Update" accept:application/json, */*,audit-id:0b0cf20a-07e5-4f36-9f83-e858a7569c6e,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Feb-2024 12:25:56.461) (total time: 644ms):
Trace[34470207]: ["GuaranteedUpdate etcd3" audit-id:0b0cf20a-07e5-4f36-9f83-e858a7569c6e,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 639ms (12:25:56.465)
Trace[34470207]:  ---"Txn call completed" 634ms (12:25:57.105)]
Trace[34470207]: [644.614314ms] [644.614314ms] END
I0208 12:50:23.464553       1 trace.go:236] Trace[1101776482]: "DeltaFIFO Pop Process" ID:kube-system/kube-scheduler-minikube,Depth:12,Reason:slow event handlers blocking the queue (08-Feb-2024 12:50:22.599) (total time: 864ms):
Trace[1101776482]: [864.956048ms] [864.956048ms] END
I0208 12:50:23.583962       1 trace.go:236] Trace[104019797]: "DeltaFIFO Pop Process" ID:v1.certificates.k8s.io,Depth:19,Reason:slow event handlers blocking the queue (08-Feb-2024 12:50:22.771) (total time: 811ms):
Trace[104019797]: [811.932983ms] [811.932983ms] END
I0208 12:50:24.003153       1 trace.go:236] Trace[1903649551]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Feb-2024 12:50:22.699) (total time: 1303ms):
Trace[1903649551]: ---"initial value restored" 1098ms (12:50:23.798)
Trace[1903649551]: ---"Transaction prepared" 92ms (12:50:23.890)
Trace[1903649551]: ---"Txn call completed" 112ms (12:50:24.002)
Trace[1903649551]: [1.303053008s] [1.303053008s] END
I0208 12:50:25.606759       1 trace.go:236] Trace[725908970]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:9e2c7966-1178-4013-8612-560f54075466,client:192.168.49.2,protocol:HTTP/2.0,resource:daemonsets,scope:resource,url:/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy/status,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:daemon-set-controller,verb:PUT (08-Feb-2024 12:50:25.091) (total time: 515ms):
Trace[725908970]: ["GuaranteedUpdate etcd3" audit-id:9e2c7966-1178-4013-8612-560f54075466,key:/daemonsets/kube-system/kube-proxy,type:*apps.DaemonSet,resource:daemonsets.apps 513ms (12:50:25.092)
Trace[725908970]:  ---"About to Encode" 264ms (12:50:25.357)
Trace[725908970]:  ---"Txn call completed" 248ms (12:50:25.606)]
Trace[725908970]: [515.648823ms] [515.648823ms] END
I0208 12:52:12.476696       1 alloc.go:330] "allocated clusterIPs" service="default/productsupportbackendapp-service" clusterIPs={"IPv4":"10.110.217.208"}

* 
* ==> kube-controller-manager [859aacedb2b2] <==
* I0208 12:25:34.137162       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="10.420005ms"
I0208 12:25:34.138667       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="1.381237ms"
I0208 12:25:34.155828       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="86.75¬µs"
I0208 12:25:35.806883       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="11.069051ms"
I0208 12:25:35.807047       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="112.954¬µs"
I0208 12:50:24.174722       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube status is now: NodeNotReady"
I0208 12:50:24.277808       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-rxvk7" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:24.371531       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="93.661747ms"
I0208 12:50:24.371615       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="44.649¬µs"
I0208 12:50:24.372225       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-ckjl7" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:24.500882       1 event.go:307] "Event occurred" object="default/my-nginx-7fbf685c4d-tsq5b" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:24.503676       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="135.201106ms"
I0208 12:50:24.503821       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="117.161¬µs"
I0208 12:50:24.667374       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68-tzbkr" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:24.676216       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/my-nginx-7fbf685c4d" duration="180.182353ms"
I0208 12:50:24.676313       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/my-nginx-7fbf685c4d" duration="59.175¬µs"
I0208 12:50:24.859383       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="192.463687ms"
I0208 12:50:24.859532       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="72.013¬µs"
I0208 12:50:24.860996       1 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:24.963966       1 event.go:307] "Event occurred" object="kube-system/kube-proxy-qt5dx" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:25.158766       1 event.go:307] "Event occurred" object="kube-system/storage-provisioner" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:25.484914       1 event.go:307] "Event occurred" object="default/productsupportbackendapp-6c46f87778-fbkpm" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:25.817549       1 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:25.873860       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="395.122222ms"
I0208 12:50:25.874019       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="87.585¬µs"
I0208 12:50:25.908585       1 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:25.989290       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7c6974c4d8-9wzk7" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:26.071087       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="96.965131ms"
I0208 12:50:26.071284       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="125.76¬µs"
I0208 12:50:26.071534       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0208 12:50:26.071937       1 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0208 12:50:31.083219       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0208 12:50:31.320345       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/my-nginx-7fbf685c4d" duration="101.810795ms"
I0208 12:50:31.320482       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/my-nginx-7fbf685c4d" duration="53.394¬µs"
I0208 12:50:31.693812       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="83.698388ms"
I0208 12:50:31.704348       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="58.317¬µs"
I0208 12:50:31.794458       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="77.141281ms"
I0208 12:50:31.794866       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="104.587¬µs"
I0208 12:50:31.931396       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="35.980413ms"
I0208 12:50:31.931470       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="44.76¬µs"
I0208 12:50:32.015533       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="92.059821ms"
I0208 12:50:32.017657       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="64.155¬µs"
I0208 12:50:32.076664       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="60.846284ms"
I0208 12:50:32.076778       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="45.813¬µs"
I0208 12:51:28.971214       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="9.639969ms"
I0208 12:51:28.971308       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="35.716¬µs"
I0208 12:51:28.993780       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="17.855757ms"
I0208 12:51:28.993977       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="46.809¬µs"
I0208 12:51:29.685937       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="95.828¬µs"
I0208 12:51:30.089033       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="46.733¬µs"
I0208 12:51:30.101339       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="52.49¬µs"
I0208 12:51:30.105480       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="30.894¬µs"
I0208 12:51:30.118061       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="7.279¬µs"
I0208 12:52:12.289507       1 event.go:307] "Event occurred" object="default/productsupportbackendapp" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set productsupportbackendapp-6c46f87778 to 1"
I0208 12:52:12.299619       1 event.go:307] "Event occurred" object="default/productsupportbackendapp-6c46f87778" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: productsupportbackendapp-6c46f87778-7s8vj"
I0208 12:52:12.325499       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="36.612091ms"
I0208 12:52:12.342515       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="16.688002ms"
I0208 12:52:12.342859       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="64.095¬µs"
I0208 12:52:14.875115       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="14.460244ms"
I0208 12:52:14.875656       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="24.986¬µs"

* 
* ==> kube-controller-manager [98517ad6c199] <==
* I0207 22:37:50.039331       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="111.581¬µs"
I0207 22:37:50.070990       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="81.604¬µs"
I0207 22:37:52.440830       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="8.935883ms"
I0207 22:37:52.440918       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="38.182¬µs"
I0207 22:44:18.481608       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="10.234014ms"
I0207 22:44:18.482579       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="74.559¬µs"
I0207 22:44:18.503117       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="13.575121ms"
I0207 22:44:18.504164       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="305.952¬µs"
I0207 22:44:19.151772       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="56.216¬µs"
I0207 22:44:19.423002       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="167.98¬µs"
I0207 22:44:19.441228       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="51.286¬µs"
I0207 22:44:19.447514       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="68.252¬µs"
I0207 22:44:19.461910       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="7.533¬µs"
I0207 22:46:26.023182       1 event.go:307] "Event occurred" object="default/productsupportbackendapp" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set productsupportbackendapp-6c46f87778 to 1"
I0207 22:46:26.037112       1 event.go:307] "Event occurred" object="default/productsupportbackendapp-6c46f87778" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: productsupportbackendapp-6c46f87778-48fdx"
I0207 22:46:26.048181       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="26.029384ms"
I0207 22:46:26.055633       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="6.837978ms"
I0207 22:46:26.055717       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="34.914¬µs"
I0207 22:46:27.007011       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="4.920812ms"
I0207 22:46:27.007079       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="25.626¬µs"
I0207 23:40:56.931503       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="34.034392ms"
I0207 23:40:56.938185       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="6.540504ms"
I0207 23:40:56.938649       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="53.813¬µs"
I0207 23:40:58.211054       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="80.995¬µs"
I0207 23:40:59.031080       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="71.853¬µs"
I0207 23:40:59.037703       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="39.905¬µs"
I0207 23:40:59.039817       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="30.24¬µs"
I0207 23:40:59.051003       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="6.046¬µs"
I0207 23:42:01.842844       1 event.go:307] "Event occurred" object="default/productsupportbackendapp" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set productsupportbackendapp-6c46f87778 to 1"
I0207 23:42:01.889865       1 event.go:307] "Event occurred" object="default/productsupportbackendapp-6c46f87778" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: productsupportbackendapp-6c46f87778-msxww"
I0207 23:42:01.898035       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="55.253278ms"
I0207 23:42:01.912469       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="13.31847ms"
I0207 23:42:01.912592       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="45.961¬µs"
I0207 23:42:02.848251       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="5.046417ms"
I0207 23:42:02.848902       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="576.781¬µs"
I0207 23:42:46.804119       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="10.424044ms"
I0207 23:42:46.804181       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="36.003¬µs"
I0207 23:42:46.818229       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="7.731807ms"
I0207 23:42:46.818321       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="39.483¬µs"
I0207 23:42:47.416579       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="60.123¬µs"
I0207 23:42:48.404913       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="32.629¬µs"
I0207 23:42:48.406753       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="81.09¬µs"
I0207 23:42:48.416055       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="5.073¬µs"
I0207 23:43:46.402390       1 event.go:307] "Event occurred" object="default/productsupportbackendapp" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set productsupportbackendapp-6c46f87778 to 1"
I0207 23:43:46.410068       1 event.go:307] "Event occurred" object="default/productsupportbackendapp-6c46f87778" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: productsupportbackendapp-6c46f87778-b96cf"
I0207 23:43:46.415269       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="12.8456ms"
I0207 23:43:46.421339       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="5.94141ms"
I0207 23:43:46.421757       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="285.545¬µs"
I0207 23:43:46.430555       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="34.579¬µs"
I0207 23:43:47.264903       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="6.795658ms"
I0207 23:43:47.265048       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="75.379¬µs"
I0207 23:44:02.073203       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="6.199147ms"
I0207 23:44:02.073528       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="179.942¬µs"
I0207 23:44:02.094610       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="9.517482ms"
I0207 23:44:02.094671       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="29.96¬µs"
I0207 23:44:02.707879       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="49.646¬µs"
I0207 23:44:03.403561       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="54.219¬µs"
I0207 23:44:03.414156       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="37.352¬µs"
I0207 23:44:03.415818       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="37.862¬µs"
I0207 23:44:03.433312       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/productsupportbackendapp-6c46f87778" duration="7.294¬µs"

* 
* ==> kube-proxy [73ccbf113d32] <==
* I0207 23:51:01.744742       1 server_others.go:69] "Using iptables proxy"
I0207 23:51:02.261757       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0207 23:51:04.763700       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0207 23:51:04.781988       1 server_others.go:152] "Using iptables Proxier"
I0207 23:51:04.782327       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0207 23:51:04.782393       1 server_others.go:438] "Defaulting to no-op detect-local"
I0207 23:51:04.849212       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0207 23:51:04.861887       1 server.go:846] "Version info" version="v1.28.3"
I0207 23:51:04.861922       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0207 23:51:04.885163       1 config.go:97] "Starting endpoint slice config controller"
I0207 23:51:04.885227       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0207 23:51:04.885315       1 config.go:188] "Starting service config controller"
I0207 23:51:04.885323       1 shared_informer.go:311] Waiting for caches to sync for service config
I0207 23:51:04.885856       1 config.go:315] "Starting node config controller"
I0207 23:51:04.885869       1 shared_informer.go:311] Waiting for caches to sync for node config
I0207 23:51:05.035916       1 shared_informer.go:318] Caches are synced for service config
I0207 23:51:05.036397       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0207 23:51:05.040180       1 shared_informer.go:318] Caches are synced for node config
I0208 06:36:43.859273       1 trace.go:236] Trace[796863427]: "iptables ChainExists" (08-Feb-2024 06:36:29.455) (total time: 12227ms):
Trace[796863427]: [12.227991664s] [12.227991664s] END
I0208 06:36:43.864096       1 trace.go:236] Trace[65141724]: "iptables ChainExists" (08-Feb-2024 06:36:31.988) (total time: 11368ms):
Trace[65141724]: [11.368844639s] [11.368844639s] END
I0208 06:36:46.582960       1 trace.go:236] Trace[2088018500]: "iptables save" (08-Feb-2024 06:36:43.868) (total time: 2714ms):
Trace[2088018500]: [2.714203751s] [2.714203751s] END
I0208 06:36:53.251843       1 trace.go:236] Trace[2059992265]: "iptables restore" (08-Feb-2024 06:36:46.584) (total time: 6689ms):
Trace[2059992265]: [6.689030107s] [6.689030107s] END
I0208 06:36:54.322833       1 trace.go:236] Trace[715199493]: "iptables restore" (08-Feb-2024 06:36:45.073) (total time: 9270ms):
Trace[715199493]: [9.270466236s] [9.270466236s] END
I0208 06:46:20.577540       1 trace.go:236] Trace[719861597]: "iptables ChainExists" (08-Feb-2024 06:46:18.419) (total time: 2157ms):
Trace[719861597]: [2.157488978s] [2.157488978s] END

* 
* ==> kube-proxy [746228c9c10e] <==
* I0207 10:01:00.290155       1 server_others.go:69] "Using iptables proxy"
I0207 10:01:00.428537       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0207 10:01:00.734052       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0207 10:01:00.739726       1 server_others.go:152] "Using iptables Proxier"
I0207 10:01:00.739805       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0207 10:01:00.739813       1 server_others.go:438] "Defaulting to no-op detect-local"
I0207 10:01:00.745019       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0207 10:01:00.746917       1 server.go:846] "Version info" version="v1.28.3"
I0207 10:01:00.746975       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0207 10:01:00.811468       1 config.go:188] "Starting service config controller"
I0207 10:01:00.811736       1 config.go:97] "Starting endpoint slice config controller"
I0207 10:01:00.814155       1 config.go:315] "Starting node config controller"
I0207 10:01:00.816938       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0207 10:01:00.818060       1 shared_informer.go:311] Waiting for caches to sync for service config
I0207 10:01:00.818195       1 shared_informer.go:311] Waiting for caches to sync for node config
I0207 10:01:00.917318       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0207 10:01:00.928322       1 shared_informer.go:318] Caches are synced for service config
I0207 10:01:00.930378       1 shared_informer.go:318] Caches are synced for node config
W0207 17:48:18.190319       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
I0207 19:50:11.305653       1 trace.go:236] Trace[1072098892]: "iptables restore" (07-Feb-2024 19:50:06.140) (total time: 5095ms):
Trace[1072098892]: [5.09568202s] [5.09568202s] END
I0207 19:50:11.307461       1 trace.go:236] Trace[1402750139]: "iptables ChainExists" (07-Feb-2024 19:50:07.347) (total time: 3864ms):
Trace[1402750139]: [3.864106743s] [3.864106743s] END
I0207 19:50:21.732067       1 trace.go:236] Trace[2058827234]: "iptables save" (07-Feb-2024 19:50:18.107) (total time: 3623ms):
Trace[2058827234]: [3.62389836s] [3.62389836s] END
I0207 19:50:27.262333       1 trace.go:236] Trace[1707563066]: "iptables restore" (07-Feb-2024 19:50:21.733) (total time: 5414ms):
Trace[1707563066]: [5.414118425s] [5.414118425s] END
I0207 20:21:07.268436       1 trace.go:236] Trace[2093018100]: "iptables ChainExists" (07-Feb-2024 20:21:04.726) (total time: 2541ms):
Trace[2093018100]: [2.541756774s] [2.541756774s] END
I0207 20:21:07.271623       1 trace.go:236] Trace[1315461213]: "iptables ChainExists" (07-Feb-2024 20:21:04.723) (total time: 2545ms):
Trace[1315461213]: [2.545090573s] [2.545090573s] END
I0207 20:21:37.277410       1 trace.go:236] Trace[1109033599]: "iptables ChainExists" (07-Feb-2024 20:21:35.093) (total time: 2182ms):
Trace[1109033599]: [2.182999879s] [2.182999879s] END
I0207 21:03:32.975378       1 trace.go:236] Trace[248588060]: "iptables ChainExists" (07-Feb-2024 21:03:31.098) (total time: 2156ms):
Trace[248588060]: [2.156145712s] [2.156145712s] END
I0207 21:33:54.740735       1 trace.go:236] Trace[1821443040]: "iptables save" (07-Feb-2024 21:33:52.131) (total time: 2609ms):
Trace[1821443040]: [2.609030973s] [2.609030973s] END
I0207 21:34:42.021843       1 trace.go:236] Trace[854761001]: "iptables restore" (07-Feb-2024 21:33:54.741) (total time: 46982ms):
Trace[854761001]: [46.982075679s] [46.982075679s] END
I0207 21:34:42.513677       1 trace.go:236] Trace[1270633661]: "iptables ChainExists" (07-Feb-2024 21:34:02.301) (total time: 40219ms):
Trace[1270633661]: [40.219178071s] [40.219178071s] END
I0207 22:18:39.322452       1 trace.go:236] Trace[745157892]: "iptables ChainExists" (07-Feb-2024 22:18:36.494) (total time: 2827ms):
Trace[745157892]: [2.827633094s] [2.827633094s] END
I0207 22:18:40.117150       1 trace.go:236] Trace[623146144]: "iptables ChainExists" (07-Feb-2024 22:18:36.893) (total time: 3224ms):
Trace[623146144]: [3.224100016s] [3.224100016s] END

* 
* ==> kube-scheduler [1f69dba09634] <==
* I0207 23:50:48.697315       1 serving.go:348] Generated self-signed cert in-memory
W0207 23:50:55.681450       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0207 23:50:55.681557       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0207 23:50:55.681606       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0207 23:50:55.681616       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0207 23:50:55.777907       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0207 23:50:55.777968       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0207 23:50:55.800937       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0207 23:50:55.801058       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0207 23:50:55.802810       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0207 23:50:55.802885       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0207 23:50:55.902807       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [28547570b9d4] <==
* I0207 10:00:53.342516       1 serving.go:348] Generated self-signed cert in-memory
W0207 10:00:56.500698       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0207 10:00:56.500748       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0207 10:00:56.500758       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0207 10:00:56.500764       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0207 10:00:56.598408       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0207 10:00:56.598474       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0207 10:00:56.610566       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0207 10:00:56.610948       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0207 10:00:56.611232       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0207 10:00:56.612303       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0207 10:00:56.713460       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0207 17:48:15.888355       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.891008       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.PersistentVolume ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.891127       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.CSINode ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.901357       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.ReplicationController ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.901432       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.PersistentVolumeClaim ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.901483       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.StorageClass ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.901526       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.CSIStorageCapacity ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.901649       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.CSIDriver ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.901729       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.ReplicaSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.902363       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.StatefulSet ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.902444       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.902526       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.902582       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:15.902632       1 reflector.go:458] vendor/k8s.io/client-go/informers/factory.go:150: watch of *v1.PodDisruptionBudget ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
W0207 17:48:16.705785       1 reflector.go:458] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
E0207 23:44:14.881341       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Feb 08 10:50:16 minikube kubelet[1490]: W0208 10:50:16.144306    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 10:55:15 minikube kubelet[1490]: W0208 10:55:15.936338    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:00:15 minikube kubelet[1490]: W0208 11:00:15.809419    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:05:15 minikube kubelet[1490]: W0208 11:05:15.599868    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:10:15 minikube kubelet[1490]: W0208 11:10:15.392687    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:17:34 minikube kubelet[1490]: E0208 11:17:33.285962    1490 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 5m45.632868524s ago; threshold is 3m0s]"
Feb 08 11:17:34 minikube kubelet[1490]: W0208 11:17:33.409399    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:17:34 minikube kubelet[1490]: E0208 11:17:33.487375    1490 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
Feb 08 11:17:34 minikube kubelet[1490]: I0208 11:17:33.866771    1490 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/productsupportbackendapp-6c46f87778-cjtpf" podStartSLOduration=8226.811493193 podCreationTimestamp="2024-02-08 09:00:27 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-02-08 09:00:30.214993115 +0000 UTC m=+33010.087682938" watchObservedRunningTime="2024-02-08 11:17:33.811493193 +0000 UTC m=+41239.497271000"
Feb 08 11:17:34 minikube kubelet[1490]: E0208 11:17:33.889793    1490 kubelet.go:2327] "Skipping pod synchronization" err="container runtime is down"
Feb 08 11:20:14 minikube kubelet[1490]: W0208 11:20:14.902984    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:25:14 minikube kubelet[1490]: W0208 11:25:14.691627    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:30:14 minikube kubelet[1490]: W0208 11:30:14.573198    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:35:14 minikube kubelet[1490]: W0208 11:35:14.363823    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:40:14 minikube kubelet[1490]: W0208 11:40:14.157137    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:45:13 minikube kubelet[1490]: W0208 11:45:13.936562    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:50:13 minikube kubelet[1490]: W0208 11:50:13.726865    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 11:55:13 minikube kubelet[1490]: W0208 11:55:13.518486    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:00:13 minikube kubelet[1490]: W0208 12:00:13.293247    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:05:13 minikube kubelet[1490]: W0208 12:05:13.083161    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:10:12 minikube kubelet[1490]: W0208 12:10:12.875021    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:15:12 minikube kubelet[1490]: W0208 12:15:12.664180    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:20:12 minikube kubelet[1490]: W0208 12:20:12.390442    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:23:49 minikube kubelet[1490]: I0208 12:23:49.496228    1490 scope.go:117] "RemoveContainer" containerID="0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57"
Feb 08 12:23:49 minikube kubelet[1490]: I0208 12:23:49.580859    1490 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kgtxp\" (UniqueName: \"kubernetes.io/projected/313b734d-af8d-42b7-9b12-62af207d04a6-kube-api-access-kgtxp\") pod \"313b734d-af8d-42b7-9b12-62af207d04a6\" (UID: \"313b734d-af8d-42b7-9b12-62af207d04a6\") "
Feb 08 12:23:49 minikube kubelet[1490]: I0208 12:23:49.583871    1490 scope.go:117] "RemoveContainer" containerID="0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57"
Feb 08 12:23:49 minikube kubelet[1490]: I0208 12:23:49.587549    1490 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/313b734d-af8d-42b7-9b12-62af207d04a6-kube-api-access-kgtxp" (OuterVolumeSpecName: "kube-api-access-kgtxp") pod "313b734d-af8d-42b7-9b12-62af207d04a6" (UID: "313b734d-af8d-42b7-9b12-62af207d04a6"). InnerVolumeSpecName "kube-api-access-kgtxp". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 08 12:23:49 minikube kubelet[1490]: E0208 12:23:49.590638    1490 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57" containerID="0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57"
Feb 08 12:23:49 minikube kubelet[1490]: I0208 12:23:49.590773    1490 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57"} err="failed to get container status \"0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57\": rpc error: code = Unknown desc = Error response from daemon: No such container: 0adfc47f7d37e2745772aa20fa6a2e0fc272e531c195c1d1ec079ed8a8c98d57"
Feb 08 12:23:49 minikube kubelet[1490]: I0208 12:23:49.682473    1490 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-kgtxp\" (UniqueName: \"kubernetes.io/projected/313b734d-af8d-42b7-9b12-62af207d04a6-kube-api-access-kgtxp\") on node \"minikube\" DevicePath \"\""
Feb 08 12:23:50 minikube kubelet[1490]: I0208 12:23:50.195709    1490 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="313b734d-af8d-42b7-9b12-62af207d04a6" path="/var/lib/kubelet/pods/313b734d-af8d-42b7-9b12-62af207d04a6/volumes"
Feb 08 12:25:12 minikube kubelet[1490]: W0208 12:25:12.171350    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:25:34 minikube kubelet[1490]: I0208 12:25:34.127935    1490 topology_manager.go:215] "Topology Admit Handler" podUID="4c78ef3b-f19e-49be-9f54-ee1fc9291478" podNamespace="default" podName="productsupportbackendapp-6c46f87778-fbkpm"
Feb 08 12:25:34 minikube kubelet[1490]: E0208 12:25:34.129619    1490 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="bbcd187c-7c07-4ac4-a1a4-01f8b7020334" containerName="productsupportbackendapp"
Feb 08 12:25:34 minikube kubelet[1490]: E0208 12:25:34.129742    1490 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="313b734d-af8d-42b7-9b12-62af207d04a6" containerName="productsupportbackendapp"
Feb 08 12:25:34 minikube kubelet[1490]: I0208 12:25:34.129913    1490 memory_manager.go:346] "RemoveStaleState removing state" podUID="bbcd187c-7c07-4ac4-a1a4-01f8b7020334" containerName="productsupportbackendapp"
Feb 08 12:25:34 minikube kubelet[1490]: I0208 12:25:34.129972    1490 memory_manager.go:346] "RemoveStaleState removing state" podUID="313b734d-af8d-42b7-9b12-62af207d04a6" containerName="productsupportbackendapp"
Feb 08 12:25:34 minikube kubelet[1490]: I0208 12:25:34.307656    1490 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mtl2x\" (UniqueName: \"kubernetes.io/projected/4c78ef3b-f19e-49be-9f54-ee1fc9291478-kube-api-access-mtl2x\") pod \"productsupportbackendapp-6c46f87778-fbkpm\" (UID: \"4c78ef3b-f19e-49be-9f54-ee1fc9291478\") " pod="default/productsupportbackendapp-6c46f87778-fbkpm"
Feb 08 12:30:11 minikube kubelet[1490]: W0208 12:30:11.957147    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:35:11 minikube kubelet[1490]: W0208 12:35:11.728360    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:40:11 minikube kubelet[1490]: W0208 12:40:11.511487    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:50:24 minikube kubelet[1490]: E0208 12:50:24.669184    1490 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 5m47.612781569s ago; threshold is 3m0s]"
Feb 08 12:50:24 minikube kubelet[1490]: E0208 12:50:24.769572    1490 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 5m47.918231712s ago; threshold is 3m0s]"
Feb 08 12:50:24 minikube kubelet[1490]: W0208 12:50:24.864403    1490 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 08 12:50:24 minikube kubelet[1490]: I0208 12:50:24.882584    1490 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/productsupportbackendapp-6c46f87778-fbkpm" podStartSLOduration=1490.882449817 podCreationTimestamp="2024-02-08 12:25:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-02-08 12:25:35.795183168 +0000 UTC m=+45324.368677308" watchObservedRunningTime="2024-02-08 12:50:24.882449817 +0000 UTC m=+46814.975346480"
Feb 08 12:50:25 minikube kubelet[1490]: E0208 12:50:25.003326    1490 kubelet.go:2327] "Skipping pod synchronization" err="[container runtime is down, PLEG is not healthy: pleg was last seen active 5m48.151051929s ago; threshold is 3m0s]"
Feb 08 12:50:25 minikube kubelet[1490]: E0208 12:50:25.495339    1490 kubelet.go:2327] "Skipping pod synchronization" err="PLEG is not healthy: pleg was last seen active 5m48.644004626s ago; threshold is 3m0s"
Feb 08 12:51:29 minikube kubelet[1490]: I0208 12:51:29.833863    1490 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mtl2x\" (UniqueName: \"kubernetes.io/projected/4c78ef3b-f19e-49be-9f54-ee1fc9291478-kube-api-access-mtl2x\") pod \"4c78ef3b-f19e-49be-9f54-ee1fc9291478\" (UID: \"4c78ef3b-f19e-49be-9f54-ee1fc9291478\") "
Feb 08 12:51:29 minikube kubelet[1490]: I0208 12:51:29.836655    1490 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4c78ef3b-f19e-49be-9f54-ee1fc9291478-kube-api-access-mtl2x" (OuterVolumeSpecName: "kube-api-access-mtl2x") pod "4c78ef3b-f19e-49be-9f54-ee1fc9291478" (UID: "4c78ef3b-f19e-49be-9f54-ee1fc9291478"). InnerVolumeSpecName "kube-api-access-mtl2x". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 08 12:51:29 minikube kubelet[1490]: I0208 12:51:29.934312    1490 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-mtl2x\" (UniqueName: \"kubernetes.io/projected/4c78ef3b-f19e-49be-9f54-ee1fc9291478-kube-api-access-mtl2x\") on node \"minikube\" DevicePath \"\""
Feb 08 12:51:30 minikube kubelet[1490]: I0208 12:51:30.077121    1490 scope.go:117] "RemoveContainer" containerID="4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213"
Feb 08 12:51:30 minikube kubelet[1490]: I0208 12:51:30.120247    1490 scope.go:117] "RemoveContainer" containerID="4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213"
Feb 08 12:51:30 minikube kubelet[1490]: E0208 12:51:30.123968    1490 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213" containerID="4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213"
Feb 08 12:51:30 minikube kubelet[1490]: I0208 12:51:30.124110    1490 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213"} err="failed to get container status \"4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213\": rpc error: code = Unknown desc = Error response from daemon: No such container: 4e34a59c47457a98a0f69b6f08eae1465b9894659e8cbc0fb3913bb762c26213"
Feb 08 12:51:30 minikube kubelet[1490]: I0208 12:51:30.957630    1490 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="4c78ef3b-f19e-49be-9f54-ee1fc9291478" path="/var/lib/kubelet/pods/4c78ef3b-f19e-49be-9f54-ee1fc9291478/volumes"
Feb 08 12:52:12 minikube kubelet[1490]: I0208 12:52:12.310488    1490 topology_manager.go:215] "Topology Admit Handler" podUID="521736d8-dbec-4544-97b2-a96268b0cb63" podNamespace="default" podName="productsupportbackendapp-6c46f87778-7s8vj"
Feb 08 12:52:12 minikube kubelet[1490]: E0208 12:52:12.310713    1490 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="4c78ef3b-f19e-49be-9f54-ee1fc9291478" containerName="productsupportbackendapp"
Feb 08 12:52:12 minikube kubelet[1490]: I0208 12:52:12.310840    1490 memory_manager.go:346] "RemoveStaleState removing state" podUID="4c78ef3b-f19e-49be-9f54-ee1fc9291478" containerName="productsupportbackendapp"
Feb 08 12:52:12 minikube kubelet[1490]: I0208 12:52:12.393387    1490 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jwhn8\" (UniqueName: \"kubernetes.io/projected/521736d8-dbec-4544-97b2-a96268b0cb63-kube-api-access-jwhn8\") pod \"productsupportbackendapp-6c46f87778-7s8vj\" (UID: \"521736d8-dbec-4544-97b2-a96268b0cb63\") " pod="default/productsupportbackendapp-6c46f87778-7s8vj"
Feb 08 12:52:13 minikube kubelet[1490]: I0208 12:52:13.799360    1490 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f38948a02ef8f894dc96e455eff330c5b435d0f7ef807c07bbe8661eb81e4c0d"

* 
* ==> kubernetes-dashboard [d50f1f5bd254] <==
* 2024/02/08 12:52:06 Getting list of all deployments in the cluster
2024/02/08 12:52:06 received 0 resources from sidecar instead of 1
2024/02/08 12:52:06 received 0 resources from sidecar instead of 1
2024/02/08 12:52:06 [2024-02-08T12:52:06Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:10 [2024-02-08T12:52:10Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/02/08 12:52:10 Getting list of namespaces
2024/02/08 12:52:10 [2024-02-08T12:52:10Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:11 [2024-02-08T12:52:11Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/02/08 12:52:11 Getting list of all deployments in the cluster
2024/02/08 12:52:11 received 0 resources from sidecar instead of 1
2024/02/08 12:52:11 received 0 resources from sidecar instead of 1
2024/02/08 12:52:11 [2024-02-08T12:52:11Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:15 [2024-02-08T12:52:15Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/02/08 12:52:15 Getting list of namespaces
2024/02/08 12:52:15 [2024-02-08T12:52:15Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:16 [2024-02-08T12:52:16Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/02/08 12:52:16 Getting list of all deployments in the cluster
2024/02/08 12:52:16 received 0 resources from sidecar instead of 2
2024/02/08 12:52:16 received 0 resources from sidecar instead of 2
2024/02/08 12:52:16 [2024-02-08T12:52:16Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:20 [2024-02-08T12:52:20Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/02/08 12:52:20 Getting list of namespaces
2024/02/08 12:52:20 [2024-02-08T12:52:20Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:21 [2024-02-08T12:52:21Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/02/08 12:52:21 Getting list of all deployments in the cluster
2024/02/08 12:52:21 received 0 resources from sidecar instead of 2
2024/02/08 12:52:21 received 0 resources from sidecar instead of 2
2024/02/08 12:52:21 [2024-02-08T12:52:21Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:25 [2024-02-08T12:52:25Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/02/08 12:52:25 Getting list of namespaces
2024/02/08 12:52:25 [2024-02-08T12:52:25Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:26 [2024-02-08T12:52:26Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/02/08 12:52:26 Getting list of all deployments in the cluster
2024/02/08 12:52:26 received 0 resources from sidecar instead of 2
2024/02/08 12:52:26 received 0 resources from sidecar instead of 2
2024/02/08 12:52:26 [2024-02-08T12:52:26Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:30 [2024-02-08T12:52:30Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/02/08 12:52:30 Getting list of namespaces
2024/02/08 12:52:30 [2024-02-08T12:52:30Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:31 [2024-02-08T12:52:31Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/02/08 12:52:31 Getting list of all deployments in the cluster
2024/02/08 12:52:31 received 0 resources from sidecar instead of 2
2024/02/08 12:52:31 received 0 resources from sidecar instead of 2
2024/02/08 12:52:31 [2024-02-08T12:52:31Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:35 [2024-02-08T12:52:35Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/02/08 12:52:35 Getting list of namespaces
2024/02/08 12:52:35 [2024-02-08T12:52:35Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:36 [2024-02-08T12:52:36Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/02/08 12:52:36 Getting list of all deployments in the cluster
2024/02/08 12:52:36 received 0 resources from sidecar instead of 2
2024/02/08 12:52:36 received 0 resources from sidecar instead of 2
2024/02/08 12:52:36 [2024-02-08T12:52:36Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:40 [2024-02-08T12:52:40Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/02/08 12:52:40 Getting list of namespaces
2024/02/08 12:52:40 [2024-02-08T12:52:40Z] Outcoming response to 127.0.0.1 with 200 status code
2024/02/08 12:52:41 [2024-02-08T12:52:41Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/02/08 12:52:41 Getting list of all deployments in the cluster
2024/02/08 12:52:41 received 0 resources from sidecar instead of 2
2024/02/08 12:52:41 received 0 resources from sidecar instead of 2
2024/02/08 12:52:41 [2024-02-08T12:52:41Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> kubernetes-dashboard [fce03fb13ee8] <==
* E0208 06:37:16.658257       1 runtime.go:77] Observed a panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod.
goroutine 38 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x16c68e0?, 0xc000fc27b0})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:75 +0x99
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x203000?})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:49 +0x75
panic({0x16c68e0, 0xc000fc27b0})
	/opt/hostedtoolcache/go/1.19.0/x64/src/runtime/panic.go:884 +0x212
github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents.func1()
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:92 +0x159
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0015c6f08?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:157 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x1c23980, 0xc00039aff0}, 0x1, 0xc000114ba0)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:158 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x0, 0x0, 0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:135 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:92
k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:83 +0x28
created by github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:88 +0x98
panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod. [recovered]
	panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod.

goroutine 38 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x203000?})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:56 +0xd7
panic({0x16c68e0, 0xc000fc27b0})
	/opt/hostedtoolcache/go/1.19.0/x64/src/runtime/panic.go:884 +0x212
github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents.func1()
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:92 +0x159
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0015c6f08?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:157 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x1c23980, 0xc00039aff0}, 0x1, 0xc000114ba0)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:158 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x0, 0x0, 0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:135 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:92
k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:83 +0x28
created by github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:88 +0x98
2024/02/08 06:37:04 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/08 06:37:06 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/08 06:37:06 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/08 06:37:06 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/08 06:37:08 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/08 06:37:08 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/08 06:37:08 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/08 06:37:10 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/08 06:37:10 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/08 06:37:10 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/08 06:37:12 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/08 06:37:12 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/08 06:37:12 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/08 06:37:14 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/08 06:37:14 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/08 06:37:14 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout

* 
* ==> storage-provisioner [affbabedbdac] <==
* sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc000390180)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc000390180, 0x18b3d60, 0xc00010cc30, 0x1, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc000390180, 0x3b9aca00, 0x0, 0xc000094501, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc000390180, 0x3b9aca00, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 121 [sync.Cond.Wait, 405 minutes]:
sync.runtime_notifyListWait(0xc000428810, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000428800)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000454060, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc00046e500, 0x18e5530, 0xc00011e740, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0003901a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0003901a0, 0x18b3d60, 0xc00010cc60, 0x1, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0003901a0, 0x3b9aca00, 0x0, 0x1, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0003901a0, 0x3b9aca00, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 122 [sync.Cond.Wait, 405 minutes]:
sync.runtime_notifyListWait(0xc000428850, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000428840)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0004541e0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc00046e500, 0x18e5530, 0xc00011e740, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0003901c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0003901c0, 0x18b3d60, 0xc00048c180, 0x1, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0003901c0, 0x3b9aca00, 0x0, 0x1, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0003901c0, 0x3b9aca00, 0xc00012a480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

* 
* ==> storage-provisioner [ccb6bce13edd] <==
* I0208 06:36:59.244872       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0208 06:36:59.366089       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0208 06:36:59.366315       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0208 06:37:16.776419       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0208 06:37:16.776748       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"58b9f321-4cb7-4b91-ac56-6943fd1b950a", APIVersion:"v1", ResourceVersion:"66138", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_cb96c261-4ff8-4631-b38f-7a621fefa442 became leader
I0208 06:37:16.778095       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_cb96c261-4ff8-4631-b38f-7a621fefa442!
I0208 06:37:16.878636       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_cb96c261-4ff8-4631-b38f-7a621fefa442!

